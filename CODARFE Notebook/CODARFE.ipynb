{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxh4aammSa1P"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKW4BXdiSdln"
      },
      "outputs": [],
      "source": [
        "# Installing requiriments\n",
        "!pip install biom-format\n",
        "!pip install scikit-bio\n",
        "\n",
        "# Imports\n",
        "from biom import load_table\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "from skbio.stats.composition import clr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import sys\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from time import perf_counter\n",
        "from multiprocessing import Process\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle as pk\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.colors as colors\n",
        "from skbio.stats.ordination import ca\n",
        "import warnings\n",
        "from IPython.display import HTML, display\n",
        "from typing import Optional, Tuple, Union\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Fts_HSd_9"
      },
      "source": [
        "# CODARFE Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SU8ax4VeR_cx"
      },
      "outputs": [],
      "source": [
        "class CODARFE():\n",
        "  \"\"\"\n",
        "  This class implements a microbiome analysis tool and prediction of continuous environmental variables associated with the microbiome.\n",
        "  It utilizes the RFE method combined with robust-to-noise regression, CoDA analysis, and 4 metrics for selecting a subgroup of the microbiome highly associated with the target variable.\n",
        "  As a result, CODARFE can predict the target variable in new microbiome samples.\n",
        "\n",
        "  CODARFE requires the following parameters:\n",
        "\n",
        "    data: pd.DataFrame = None\n",
        "          DataFrame containing count data (microbiome);\n",
        "    metaData: pd.DataFrame = None\n",
        "          DataFrame containing the target variable related to the microbiome;\n",
        "    target: str = None\n",
        "          Name of the column in metaData that contains the target variable.\n",
        "\n",
        "  Usage:\n",
        "      1) Create an instance of CODARFE with your data:\n",
        "\n",
        "      coda = CODARFE(data       = <microbiome_dataframe>,\n",
        "                     metaData   = <metadata_dataframe>,\n",
        "                     target = <string_target_variable_name>)\n",
        "\n",
        "      2) Train a model:\n",
        "\n",
        "      coda.CreateModel( write_results            = True,\n",
        "                        path_out                 = '',\n",
        "                        name_append              = '',\n",
        "                        rLowVar                  = True,\n",
        "                        applyAbunRel             = True,\n",
        "                        percentage_cols_2_remove = 1,\n",
        "                        n_Kfold_CV               = 10,\n",
        "                        weightR2                 = 1.0,\n",
        "                        weightProbF              = 0.5,\n",
        "                        weightBIC                = 1.0,\n",
        "                        weightRMSE               = 1.5,\n",
        "                        n_max_iter_huber         = 100\n",
        "                     )\n",
        "\n",
        "\n",
        "      3) Save the model:\n",
        "\n",
        "      coda.save_instance(path_out    = <path_to_folder>,\n",
        "                         name_append = <name>)\n",
        "\n",
        "\n",
        "      Alternatively, you can load a pre-trained model\n",
        "\n",
        "\n",
        "      coda = CODARFE()\n",
        "      coda.load_instance(path2instance = <path_to_file_instance.foda>)\n",
        "\n",
        "\n",
        "\n",
        "      4) View the results:\n",
        "\n",
        "        4.1) Plot of predicted vs. expected correlation\n",
        "\n",
        "        coda.Plot_Correlation(path_out    = <path_to_folder>,\n",
        "                              name_append = <name>)\n",
        "\n",
        "        4.2) Plot the mean absolute error using a hold-out validation\n",
        "\n",
        "        coda.Plot_HoldOut_Validation( n_repetitions = 100,\n",
        "                                      test_size     = 20,\n",
        "                                      path_out      = <path_to_folder>,\n",
        "                                      name_append   = <name>)\n",
        "\n",
        "        4.3) Plot of the relationship of predictors with the target\n",
        "\n",
        "        coda.Plot_Relevant_Predictors(n_max_features = 100,\n",
        "                                      path_out       = <path_to_folder>,\n",
        "                                      name_append    = <name>)\n",
        "\n",
        "        4.4) Heatmap of selected predictors\n",
        "\n",
        "        coda.Plot_Heatmap(path_out    = <path_to_folder>,\n",
        "                          name_append = <name>)\n",
        "\n",
        "        4.5) Selected predictors\n",
        "\n",
        "        coda.selected_taxa\n",
        "\n",
        "      5) Predict the target variable in new samples:\n",
        "\n",
        "      coda.Predict( path2newdata = <path_to_new_data>,\n",
        "                    applyAbunRel = True,\n",
        "                    writeResults = True,\n",
        "                    path_out     = <path_out>\n",
        "                    name_append  = <name>)\n",
        "\n",
        "  For more information about the tool, visit the original publication or the GitHub containing more versions of this same tool.\n",
        "\n",
        "  For questions, suggestions, or bug/error reports, contact via email: murilobarbosa@alunos.utfpr.edu.br\"\n",
        "\n",
        "  \"\"\"\n",
        "  class ModelNotCreatedError(Exception):\n",
        "    def __init__(self, mensagem=\"No model created! Please create the model using the CreateModel function and try again.\"):\n",
        "            self.mensagem = mensagem\n",
        "            super().__init__(self.mensagem)\n",
        "\n",
        "  class EmptyDataError(Exception):\n",
        "    def __init__(self, mensagem=\"No model created! Please create the model using the CreateModel function and try again.\"):\n",
        "            self.mensagem = mensagem\n",
        "            super().__init__(self.mensagem)\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self,\n",
        "               path_to_data: Optional[str] = None,\n",
        "               path_to_metadata: Optional[str] = None,\n",
        "               target: Optional[str] = None,\n",
        "               flag_first_col_as_index_data: bool = False,\n",
        "               flag_first_col_as_index_metaData: bool = False\n",
        "               ) -> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_to_data: str = None\n",
        "          The path to the microbiome dataframe (counting table)\n",
        "    flag_first_col_as_index_data: bool = False\n",
        "          If True, the first row of the dataframe will be used as index\n",
        "    path_to_metadata: str = None\n",
        "          The path to the metadata dataframe with the target variable\n",
        "    flag_first_col_as_index_metaData: bool = False\n",
        "          If True, the first row of the dataframe will be used as index\n",
        "    target: str = None\n",
        "          The name of the target variable column inside the metadata\n",
        "    \"\"\"\n",
        "    self.__flag_first_col_as_index_data = flag_first_col_as_index_data\n",
        "    self.__flag_first_col_as_index_metaData = flag_first_col_as_index_metaData\n",
        "    self.__path_to_metadata = path_to_metadata\n",
        "    if path_to_data != None and path_to_metadata != None and target != None:\n",
        "      print(\"Loading data... It may take some minutes depending on the size of the data\")\n",
        "      self.data, self.target = self.__read_data(path_to_data,path_to_metadata,target)\n",
        "      self.__totalPredictorsInDatabase = len(self.data.columns)\n",
        "    else:\n",
        "      self.data = None\n",
        "      self.target = None\n",
        "      # print('No complete data provided. Please use the function load_instance(<path_2_instance>) to load an already created CODARFE model.')\n",
        "\n",
        "    self.__sqrt_transform = None\n",
        "    self.__transform = None\n",
        "    self.__min_target_sqrt_transformed = None\n",
        "    self.__max_target_sqrt_transformed = None\n",
        "    self.__min_target_transformed = None\n",
        "    self.__max_target_transformed = None\n",
        "    self.results = None\n",
        "    self.score_best_model = None\n",
        "    self.selected_taxa = None\n",
        "    self.__model = None\n",
        "    self.__n_max_iter_huber = None\n",
        "    self.__applyAbunRel = True\n",
        "\n",
        "    self.__correlation_list = {}\n",
        "\n",
        "  def __read_data(self,path_to_data,path_to_metadata,target_column_name):\n",
        "    if not os.path.exists(path_to_data):\n",
        "      print('The Data file does not exists!')\n",
        "      raise FileNotFoundError('The Data file does not exists!')\n",
        "    if not os.path.exists(path_to_metadata):\n",
        "      raise FileNotFoundError('The Metadata file does not exists!')\n",
        "    extension = path_to_data.split('.')[-1]\n",
        "    if extension == 'csv':\n",
        "        data = pd.read_csv(path_to_data,encoding='latin1')\n",
        "        if self.__flag_first_col_as_index_data:\n",
        "            data.set_index(list(data.columns)[0],inplace=True)\n",
        "    elif extension == 'tsv':\n",
        "        data = pd.read_csv(path_to_data,sep='\\t',encoding='latin1')\n",
        "        if self.__flag_first_col_as_index_data:\n",
        "            data.set_index(list(data.columns)[0],inplace=True)\n",
        "    elif extension == 'biom':\n",
        "        table = load_table(path_to_data)\n",
        "        data = table.to_dataframe()\n",
        "    elif extension == 'qza':\n",
        "        output_directory =  '/'.join(path_to_data.split('/')[:-1])+'/QZA_EXTRACT_CODARFE_TEMP/'\n",
        "        # Openning the .qza file as an zip file\n",
        "        with zipfile.ZipFile(path_to_data, 'r') as zip_ref:\n",
        "            # extracting all data to the output directory\n",
        "            zip_ref.extractall(output_directory)\n",
        "        # Getting the biom path file\n",
        "        biompath = output_directory+os.listdir(output_directory)[0]+'/data/'\n",
        "        biompath += [f for f in os.listdir(biompath) if f[-5:]=='.biom'][0]\n",
        "        table = load_table(biompath)# Read the biom file\n",
        "        data = table.to_dataframe() # Tranform it to a pandas dataframe\n",
        "\n",
        "        shutil.rmtree(output_directory) # remove the pathTree created\n",
        "\n",
        "    extension = path_to_metadata.split('.')[-1]\n",
        "    if extension == 'csv':\n",
        "        metadata = pd.read_csv(path_to_metadata,encoding='latin1')\n",
        "        if self.__flag_first_col_as_index_metaData:\n",
        "            metadata.set_index(list(metadata.columns)[0],inplace=True)\n",
        "    elif extension == 'tsv':\n",
        "        metadata = pd.read_csv(path_to_metadata,sep='\\t',encoding='latin1')\n",
        "        if self.__flag_first_col_as_index_metaData:\n",
        "            metadata.set_index(list(metadata.columns)[0],inplace=True)\n",
        "\n",
        "    totTotal = len(metadata)\n",
        "    if target_column_name not in metadata.columns:\n",
        "      raise ValueError(\"The Target is not present in the metadata table!\")\n",
        "    totNotNa = metadata[target_column_name].isna().sum()\n",
        "    metadata.dropna(subset=[target_column_name],inplace=True)\n",
        "\n",
        "\n",
        "    data = data.loc[metadata.index]\n",
        "    y = metadata[target_column_name]\n",
        "\n",
        "    if len(data) == 0:\n",
        "      print('There is no correspondence between the ids of the predictors and the metadata.\\nMake sure the column corresponding to the identifiers is first.')\n",
        "      sys.exit(1)\n",
        "    print('Total samples with the target variable: ',totTotal-totNotNa,'/',totTotal)\n",
        "\n",
        "    return data,y\n",
        "\n",
        "  def save_instance(self,path_out: str,name_append: Optional[str]='CODARFE_MODEL')-> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_out: str\n",
        "              Path to folder where it will be saved. If no path is provided it will save in the same directory as the metadata with the name of 'CODARFE_MODEL.foda'\n",
        "    name_append: str = 'CODARFE_MODEL'\n",
        "              Name to concatenate in the final filename.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "              If de path_out does not exists\n",
        "\n",
        "    \"\"\"\n",
        "    if type(self.data) == type(None):\n",
        "      print('Nothing to save.')\n",
        "      return\n",
        "\n",
        "    if '/' in path_out:\n",
        "      path2 = '/'.join(path_out.split('/')[:-1])\n",
        "      if not os.path.exists(path2):\n",
        "        raise FileNotFoundError(\"The path out does not exists!\")\n",
        "\n",
        "\n",
        "    if path_out != '':\n",
        "      if path_out[-1]!= '/':\n",
        "        path_out+='/'\n",
        "    else:\n",
        "      path_out = '/'.join(self.__path_to_metadata.split('/')[:-1])+'/'\n",
        "    # adiciona '_' caso n tenha\n",
        "    if name_append != '':\n",
        "      if name_append[0]!= '_':\n",
        "        name_append = 'CODARFE_MODEL_'+name_append\n",
        "    else:\n",
        "      name_append = 'CODARFE_MODEL'\n",
        "    filename = path_out+name_append+'.foda'\n",
        "\n",
        "    obj = {'data':self.data,\n",
        "           'target':self.target,\n",
        "           'sqrt_transform': self.__sqrt_transform,\n",
        "           'transform': self.__transform,\n",
        "           'min_target_sqrt_transformed': self.__min_target_sqrt_transformed,\n",
        "           'max_target_sqrt_transformed': self.__max_target_sqrt_transformed,\n",
        "           'min_target_transformed': self.__min_target_transformed,\n",
        "           'max_target_transformed': self.__max_target_transformed,\n",
        "           'results': self.results,\n",
        "           'score_best_model': self.score_best_model,\n",
        "           'selected_taxa': self.selected_taxa,\n",
        "           'model': self.__model,\n",
        "           'n_max_iter_huber': self.__n_max_iter_huber,\n",
        "           'correlation_list':self.__correlation_list}\n",
        "\n",
        "\n",
        "    with open(filename,'wb') as f:\n",
        "      pk.dump(obj,f)\n",
        "\n",
        "    print('\\n\\nInstance saved at ',filename,'\\n\\n')\n",
        "\n",
        "  def load_instance(self,path2instance: str) -> None:\n",
        "    \"\"\"\n",
        "    Load the CODARFE instance stored in the <path2instance> file into this object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path2instance: str\n",
        "                   Path to \".foda\" file\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "                   If the path2instance does not exists\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path2instance):\n",
        "      raise FileNotFoundError(f\"The file {path2instance} does not exists\")\n",
        "\n",
        "    with open(path2instance,'rb') as f:\n",
        "      obj = pk.load(f)\n",
        "\n",
        "    self.data = obj['data']\n",
        "    self.target = obj['target']\n",
        "    self.__sqrt_transform = obj['sqrt_transform']\n",
        "    self.__transform = obj['transform']\n",
        "    self.__min_target_sqrt_transformed = obj['min_target_sqrt_transformed']\n",
        "    self.__max_target_sqrt_transformed = obj['max_target_sqrt_transformed']\n",
        "    self.__min_target_transformed = obj['min_target_transformed']\n",
        "    self.__max_target_transformed = obj['max_target_transformed']\n",
        "    self.results = obj['results']\n",
        "    self.score_best_model = obj['score_best_model']\n",
        "    self.selected_taxa = obj['selected_taxa']\n",
        "    self.__model = obj['model']\n",
        "    self.__n_max_iter_huber = obj['n_max_iter_huber']\n",
        "    self.__correlation_list = obj['correlation_list']\n",
        "\n",
        "    print('\\n\\nInstance restored successfully!\\n\\n')\n",
        "\n",
        "\n",
        "  def __remove_low_var(self):\n",
        "    aux = self.data.copy()\n",
        "    cols = aux.columns\n",
        "    selector = VarianceThreshold(aux.var(axis=1).mean()/8)#8\n",
        "    aux = selector.fit(aux)\n",
        "    not_to_drop=list(cols[selector.get_support()])\n",
        "    totRemoved = len(self.data.columns) - len(not_to_drop)\n",
        "    print('\\nA total of ',totRemoved,' taxa were removed due to very low variance\\n')\n",
        "    self.data =  self.data[not_to_drop]\n",
        "\n",
        "  def __to_abun_rel(self,data):\n",
        "    return data.apply(lambda x: x/x.sum() if x.sum()!=0 else x,axis=1)\n",
        "\n",
        "  def __calc_new_redimension(self,target):\n",
        "\n",
        "    minimo = min(target)\n",
        "    maximo = max(target)\n",
        "    if self.__min_target_transformed == None or self.__max_target_transformed == None:\n",
        "      self.__min_target_transformed = minimo\n",
        "      self.__max_target_transformed = maximo\n",
        "\n",
        "    numeros_redimensionados = [x + abs(self.__min_target_transformed)+1 for x in target]\n",
        "    return numeros_redimensionados\n",
        "\n",
        "  def __calc_inverse_redimension(self,predictions):\n",
        "\n",
        "    numeros_restaurados = [x - abs(self.__min_target_transformed)-1 for x in predictions]\n",
        "\n",
        "    return numeros_restaurados\n",
        "\n",
        "  def __calc_new_sqrt_redimension(self,target):\n",
        "    target = target.apply(lambda x: np.sqrt(abs(x)) * (-1 if x < 0 else 1))\n",
        "\n",
        "    minimo = min(target)\n",
        "    maximo = max(target)\n",
        "    if self.__min_target_sqrt_transformed == None or self.__max_target_sqrt_transformed == None:\n",
        "      self.__min_target_sqrt_transformed = minimo\n",
        "      self.__max_target_sqrt_transformed = maximo\n",
        "\n",
        "    new_min = 0  # novo valor mínimo desejado\n",
        "    new_max = 100  # novo valor máximo desejado\n",
        "    # numeros_redimensionados = [x + abs(self.__min_target_sqrt_transformed)+1 for x in target]\n",
        "    numeros_redimensionados = [(x - minimo) / (maximo - minimo) * (new_max - new_min) + new_min for x in target]\n",
        "    return numeros_redimensionados\n",
        "\n",
        "  def __calc_inverse_sqrt_redimension(self,predictions):\n",
        "    new_min = 0  # novo valor mínimo usado na transformação\n",
        "    new_max = 100  # novo valor máximo usado na transformação\n",
        "\n",
        "    minimo = self.__min_target_sqrt_transformed\n",
        "    maximo = self.__max_target_sqrt_transformed\n",
        "\n",
        "    numeros_restaurados = [(x - new_min) / (new_max - new_min) * (maximo - minimo) + minimo for x in predictions]\n",
        "\n",
        "    # numeros_restaurados = [x + abs(self.__min_target_sqrt_transformed)+1 for x in predictions]\n",
        "    numeros_restaurados_sqrt_inverse = [(x**2) * (-1 if x <0 else 1) for x in numeros_restaurados]\n",
        "    return numeros_restaurados_sqrt_inverse\n",
        "\n",
        "  def __to_CLR(self,df): # Transform to CLr\n",
        "    aux = df.copy()\n",
        "    cols = aux.columns\n",
        "    aux+=0.0001 # Pseudo count\n",
        "    aux = clr(aux)\n",
        "    aux = pd.DataFrame(data=aux,columns=cols)\n",
        "    return aux\n",
        "\n",
        "  def __calc_mse_model_centeredv2(self,pred,y,df_model): #pred é o valor predito  # df_model é o número de variáveis\n",
        "    mean = np.mean(y)\n",
        "    ssr = sum([(yy-pp)**2 for yy,pp in zip(y,pred)])\n",
        "    centered_tss = sum([(aux-mean)**2 for aux in y])\n",
        "    ess = centered_tss - ssr\n",
        "    return ess/df_model\n",
        "\n",
        "  def __calc_mse_resid(self,pred,y,df_resid):\n",
        "    residuos = [yy-pp for yy,pp in zip(y,pred)]\n",
        "    return sum([aux**2 for aux in residuos])/df_resid\n",
        "\n",
        "  def __calc_f_prob_centeredv2(self,pred,y,X):\n",
        "    df_model = max(1,len(X.iloc[0]) - 1)\n",
        "    df_resid = max(1,len(X) - df_model -1 )\n",
        "    mse_model = self.__calc_mse_model_centeredv2(pred,y,df_model)\n",
        "    mse_resid = self.__calc_mse_resid(pred,y,df_resid)\n",
        "    fstatistic = mse_model/mse_resid\n",
        "    return stats.f.sf(fstatistic, df_model, df_resid)\n",
        "\n",
        "  def __calc_r_squared(self,pred,y,method = 'centered'):\n",
        "    ssr = sum([(yy-pp)**2 for yy,pp in zip(y,pred)])\n",
        "    mean = np.mean(y)\n",
        "    center_tss = np.sum((y - mean)**2)\n",
        "    uncentered_tss = sum([(aux)**2 for aux in y])\n",
        "    if method == 'centered':\n",
        "      return 1 - ssr/center_tss\n",
        "    else:\n",
        "      return 1 - ssr/uncentered_tss\n",
        "\n",
        "  def __calc_rsquared_adj(self,X,r2):\n",
        "    return 1 - (1-r2) * (X.shape[0]-1)/(X.shape[0]-X.shape[1]-1)\n",
        "\n",
        "  def __calc_llf(self,pred,y,X):\n",
        "    nobs2 = len(X) / 2.0\n",
        "    nobs = float(len(X))\n",
        "    ssr = sum([(yy-pp)**2 for yy,pp in zip(y,pred)])\n",
        "    llf = -nobs2*np.log(2*np.pi) - nobs2*np.log(ssr / nobs) - nobs2\n",
        "    return llf\n",
        "\n",
        "\n",
        "  def __calc_bic(self,pred,y,X):\n",
        "    llf = self.__calc_llf(pred,y,X)\n",
        "    nobs = float(len(X))\n",
        "    k_params = len(X.iloc[0])\n",
        "    bic = -2*llf + np.log(nobs) * k_params\n",
        "    return round(bic)\n",
        "\n",
        "  def __progress(self,value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "  def __super_RFE(self,method,n_cols_2_remove,n_Kfold_CV):\n",
        "    # Define o total de atributos a serem removidos por rodada\n",
        "    n_cols_2_remove = max([int(len(self.data.columns)*n_cols_2_remove),1])\n",
        "\n",
        "    # Define X inicial como descritores\n",
        "    if self.__applyAbunRel:\n",
        "      X_train = self.__to_abun_rel(self.data)\n",
        "    else:\n",
        "      X_train = self.data\n",
        "\n",
        "    tot2display = len(list(X_train.columns))\n",
        "    # Define variável alvo\n",
        "    if self.__sqrt_transform:\n",
        "      y_train = np.array(self.__calc_new_sqrt_redimension(self.target))\n",
        "    elif self.__transform:\n",
        "      y_train = np.array(self.__calc_new_redimension(self.target))\n",
        "    else:\n",
        "      y_train = self.target\n",
        "\n",
        "    # Inicializa tabela de resultados\n",
        "    resultTable = pd.DataFrame(columns=['Atributos','R² adj','F-statistic','BIC','MSE-CV'])\n",
        "    percentagedisplay = round(100 - (len(list(X_train.columns))/tot2display)*100)\n",
        "    out = display(self.__progress(0, 100), display_id=True)\n",
        "    while len(X_train.columns) - n_cols_2_remove > n_cols_2_remove or len(X_train.columns) > 1:\n",
        "      X = self.__to_CLR(X_train)\n",
        "\n",
        "      method.fit(X,y_train)\n",
        "\n",
        "      pred = method.predict(X)\n",
        "\n",
        "      Fprob = self.__calc_f_prob_centeredv2(pred,y_train,X)\n",
        "\n",
        "      r2 = self.__calc_r_squared(pred,y_train)\n",
        "      r2adj = self.__calc_rsquared_adj(X,r2)\n",
        "\n",
        "      BIC = self.__calc_bic(pred,y_train,X)\n",
        "\n",
        "      msecv_results = []\n",
        "      #Adicionando etapa de validação cruzada\n",
        "      kf = KFold(n_splits=n_Kfold_CV,shuffle=True,random_state=42)\n",
        "\n",
        "      for train, test in kf.split(X):\n",
        "\n",
        "        # Not sure why sometimes it is a Series... but I hope this will solve the problem...\n",
        "        # When it is Series needs to use \"iloc[]\"\n",
        "        if isinstance(y_train,pd.core.series.Series):\n",
        "          model = method.fit(X.iloc[train],y_train.iloc[train])\n",
        "\n",
        "          predcv = model.predict(X.iloc[test])\n",
        "\n",
        "          msecv_results.append(np.mean([(t-p)**2 for t,p in zip(y_train.iloc[test],predcv)])**(1/2))\n",
        "        else:\n",
        "          model = method.fit(X.iloc[train],y_train[train])\n",
        "\n",
        "          predcv = model.predict(X.iloc[test])\n",
        "\n",
        "          msecv_results.append(np.mean([(t-p)**2 for t,p in zip(y_train[test],predcv)])**(1/2))\n",
        "\n",
        "\n",
        "      msecv = np.mean(msecv_results)\n",
        "\n",
        "      # caso consiga calcular um p-valor\n",
        "      if not math.isnan(Fprob) and r2adj < 1.0:\n",
        "        # Cria linha com: Atributos separados por ';', R² ajustado, estatistica F e estatistica BIC\n",
        "        newline = pd.DataFrame.from_records([{'Atributos':'@'.join(list(X.columns)),'R² adj':float(r2adj),'F-statistic':float(Fprob),'BIC':float(BIC),'MSE-CV':float(msecv)}])\n",
        "\n",
        "        # Adiciona linha na tabela de resultado\n",
        "        resultTable = pd.concat([resultTable,newline])\n",
        "\n",
        "      # Cria tabela de dados dos atributos\n",
        "      atr = pd.DataFrame(data = [[xx,w] for xx,w in zip(X.columns,method.coef_)],columns = ['Atributos','coef'])\n",
        "\n",
        "      # Transforma coluna de coeficiente p/ float\n",
        "      atr = atr.astype({'coef':float})\n",
        "\n",
        "      # Remove colunas com coeficientes iguais ou menores que 0 / Acelera o rfe\n",
        "      atr = atr[atr.coef != 0]# Remove zeros\n",
        "      atr.coef = atr.coef.abs()#Transforma em abs para remover apenas os X% mais perto de zero\n",
        "\n",
        "      # Ordena de forma decrescente pelo coeficiente\n",
        "      atr = atr.sort_values(by=['coef'],ascending=False)\n",
        "\n",
        "      # Cria lista de atributos selecionados\n",
        "      atributos = list(atr.Atributos)\n",
        "\n",
        "      # Remove espaços em brancos inseridos pela tabela de atributos\n",
        "      atributos = [x.strip() for x in atributos]\n",
        "\n",
        "      # Remove n_cols_2_remove menos relevantes\n",
        "      atributos = atributos[:-n_cols_2_remove]\n",
        "\n",
        "      # Remove atributos n selecionados\n",
        "      if self.__applyAbunRel:\n",
        "        X_train = self.__to_abun_rel(self.data[atributos])\n",
        "      else:\n",
        "        X_train = X_train[atributos]\n",
        "\n",
        "      # Calculo da % para mostrar na tela\n",
        "      percentagedisplay = round(100 - (len(list(X_train.columns))/tot2display)*100)\n",
        "      # print(percentagedisplay,'% done...\\n')\n",
        "      out.update(self.__progress(int(percentagedisplay), 100))\n",
        "    #Remove possiveis nan\n",
        "    resultTable.dropna(inplace=True)\n",
        "    # print('100% done!\\n')\n",
        "    out.update(self.__progress(100, 100))\n",
        "    # Retorna a tabela com os resultados\n",
        "    return resultTable\n",
        "\n",
        "  def __score_and_selection(self,resultTable,weightR2,weightProbF,weightBIC,weightRMSE):\n",
        "    # Cria cópia da tabela original\n",
        "    df_aux = resultTable.copy()\n",
        "    df_aux.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_aux.dropna(inplace=True)\n",
        "\n",
        "    # Normaliza o R² ajustado\n",
        "    df_aux['R² adj'] = MinMaxScaler().fit_transform(np.array(df_aux['R² adj']).reshape(-1,1))\n",
        "\n",
        "    # Aplica a transformação -log10(f) e então normaliza a estatistica f\n",
        "    df_aux['F-statistic'] = [-math.log10(x) if x !=0 else sys.float_info.max for x in df_aux['F-statistic']]\n",
        "    df_aux['F-statistic'] = MinMaxScaler().fit_transform(np.array(df_aux['F-statistic']).reshape(-1,1))\n",
        "\n",
        "    # Normaliza e então inverte a estatistica BIC (Quanto menor menor)\n",
        "    df_aux['BIC'] = MinMaxScaler().fit_transform(np.array(df_aux['BIC']).reshape(-1,1))\n",
        "    df_aux['BIC'] = [np.clip(1-x,0,1) for x in df_aux['BIC']]\n",
        "\n",
        "    # Normaliza 'MSE-CV' e inverte a estatistica MSE-cv\n",
        "    df_aux['MSE-CV'] = MinMaxScaler().fit_transform(np.array(df_aux['MSE-CV']).reshape(-1,1))\n",
        "    df_aux['MSE-CV'] = [np.clip(1-x,0,1) for x in df_aux['MSE-CV']]\n",
        "\n",
        "    # Cria coluna de Score\n",
        "    df_aux['Score'] = [(r*weightR2)+(f*weightProbF)+(b*weightBIC)+(m*weightRMSE) for r,f,b,m in zip(df_aux['R² adj'],df_aux['F-statistic'],df_aux['BIC'],df_aux['MSE-CV'])]\n",
        "\n",
        "    # Encontra indice de maior Score\n",
        "    indexSelected = list(df_aux.Score).index(max(list(df_aux.Score)))\n",
        "\n",
        "    # Seleciona atributos\n",
        "    selected = df_aux.iloc[indexSelected].Atributos.split('@')\n",
        "\n",
        "    self.selected_taxa = selected # salva os atributos selecionados\n",
        "\n",
        "    retEstatisticas = list(resultTable.iloc[indexSelected][['R² adj','F-statistic','BIC','MSE-CV']])\n",
        "\n",
        "    self.results = {'R² adj':retEstatisticas[0],\n",
        "                    'F-statistic':retEstatisticas[1],\n",
        "                    'BIC':retEstatisticas[2],\n",
        "                    'MSE-CV':retEstatisticas[3]} # Salva as estatisticas\n",
        "\n",
        "    retScore = df_aux.iloc[indexSelected].Score\n",
        "\n",
        "    self.score_best_model = retScore # Salva a pontuação do melhor modelo\n",
        "\n",
        "  def __write_results(self,path_out,name_append):\n",
        "    # adiciona '/' caso n tneha\n",
        "    if path_out != '':\n",
        "      if path_out[-1]!= '/':\n",
        "        path_out+='/'\n",
        "    else:\n",
        "      path_out = '/'.join(self.__path_to_metadata.split('/')[:-1])+'/'\n",
        "    # adiciona '_' caso n tenha\n",
        "    if name_append != '':\n",
        "      if name_append[0]!= '_':\n",
        "        name_append = 'CODARFE_RESULTS_'+name_append\n",
        "    else:\n",
        "      name_append = 'CODARFE_RESULTS'\n",
        "\n",
        "    path2write = path_out +name_append+'.txt'\n",
        "    print('Writing results at ',path2write)\n",
        "\n",
        "    with open(path2write,'w') as f:\n",
        "      f.write('Results: \\n\\n')\n",
        "      f.write('R² adj -> '+     str(self.results['R² adj'])+'\\n')\n",
        "      f.write('F-statistic -> '+str(self.results['F-statistic'])+'\\n')\n",
        "      f.write('BIC -> '+        str(self.results['BIC'])+'\\n')\n",
        "      f.write('MSE-CV -> '+     str(self.results['MSE-CV'])+'\\n')\n",
        "      f.write('Total of taxa selected -> '+str(len(self.selected_taxa))+'. This value corresponds to '+str((len(self.selected_taxa)/len(self.data.columns))*100)+'% of the total observed\\n\\n')\n",
        "      f.write('Selected taxa: \\n\\n')\n",
        "      f.write(','.join(self.selected_taxa))\n",
        "\n",
        "  def __define_model(self,allow_transform_high_variation):\n",
        "\n",
        "      self.__model = RandomForestRegressor(n_estimators = 160, criterion = 'poisson',random_state=42)\n",
        "      X = self.data[self.selected_taxa]\n",
        "      if self.__applyAbunRel:\n",
        "        X = self.__to_abun_rel(X)\n",
        "      X = self.__to_CLR(X)\n",
        "\n",
        "      if allow_transform_high_variation and np.std(self.target)/np.mean(self.target)>0.2 :# Caso varie muitas vezes a média (ruido)\n",
        "        targetLogTransformed = self.__calc_new_sqrt_redimension(self.target) # Aplica transformação no alvo\n",
        "        self.__model.fit(X,targetLogTransformed) # Treina com o alvo transformado\n",
        "        self.__sqrt_transform = True # Define flag de transformação\n",
        "        self.__transform = False\n",
        "\n",
        "      else:\n",
        "        if any(t < 0 for t in self.target):\n",
        "          self.__transform = True\n",
        "          targetTranformed = self.__calc_new_redimension(self.target)\n",
        "          self.__model.fit(X,targetTranformed)\n",
        "          self.__sqrt_transform = False\n",
        "          print(f\"The data was shifted {abs(self.__min_target_transformed)} + 1 units due to negative values not supported by poisson distribution.\")\n",
        "\n",
        "        else:\n",
        "          self.__model.fit(X,self.target) # Treina um segundo modelo com o alvo como é\n",
        "          self.__sqrt_transform = False # Define flag de transformação\n",
        "          self.__transform = False\n",
        "\n",
        "\n",
        "  def __check_boolean(self,value, name):\n",
        "      if not isinstance(value, bool):\n",
        "          raise ValueError(f\"{name} must be a boolean.\")\n",
        "\n",
        "  def __check_string(self,value, name):\n",
        "      if not isinstance(value, str):\n",
        "          raise ValueError(f\"{name} must be a string.\")\n",
        "\n",
        "  def __check_integer_range(self,value, name, min_value, max_value):\n",
        "      if value < min_value or value > max_value:\n",
        "          raise ValueError(f\"{name} must be between {min_value} and {max_value}.\")\n",
        "\n",
        "  def __check_integer(self,value, name):\n",
        "      if not isinstance(value, int):\n",
        "          raise ValueError(f\"{name} must be an integer.\")\n",
        "\n",
        "  def __check_non_negative_float(self,value, name):\n",
        "      if not isinstance(value, float):\n",
        "          raise ValueError(f\"{name} must be a float.\")\n",
        "      elif value < 0:\n",
        "          raise ValueError(f\"{name} must be greater than or equal to 0.\")\n",
        "\n",
        "  def __check_model_params(self,\n",
        "                         write_results,\n",
        "                         path_out,\n",
        "                         name_append,\n",
        "                         rLowVar,\n",
        "                         applyAbunRel,\n",
        "                         allow_transform_high_variation,\n",
        "                         percentage_cols_2_remove,\n",
        "                         n_Kfold_CV,\n",
        "                         weightR2,\n",
        "                         weightProbF,\n",
        "                         weightBIC,\n",
        "                         weightRMSE,\n",
        "                         n_max_iter_huber):\n",
        "      self.__check_boolean(write_results, \"write_results\")\n",
        "      self.__check_boolean(rLowVar, \"rLowVar\")\n",
        "      self.__check_boolean(applyAbunRel, \"applyAbunRel\")\n",
        "      self.__check_boolean(allow_transform_high_variation, \"allow_transform_high_variation\")\n",
        "      self.__check_string(path_out, \"path_out\")\n",
        "      if write_results and path_out != '' and not os.path.exists(path_out):\n",
        "          raise FileNotFoundError(\"The path out does not exist!\")\n",
        "      self.__check_string(name_append, \"name_append\")\n",
        "      self.__check_integer(percentage_cols_2_remove, \"percentage_cols_2_remove\")\n",
        "      self.__check_integer_range(percentage_cols_2_remove, \"percentage_cols_2_remove\", 1, 99)\n",
        "      self.__check_integer(n_Kfold_CV, \"n_Kfold_CV\")\n",
        "      self.__check_integer_range(n_Kfold_CV, \"n_Kfold_CV\", 2, 100)\n",
        "      self.__check_integer(n_max_iter_huber, \"n_max_iter_huber\")\n",
        "      self.__check_integer_range(n_max_iter_huber, \"n_max_iter_huber\", 2, 1000)\n",
        "      self.__check_non_negative_float(weightR2, \"weightR2\")\n",
        "      self.__check_non_negative_float(weightProbF, \"weightProbF\")\n",
        "      self.__check_non_negative_float(weightBIC, \"weightBIC\")\n",
        "      self.__check_non_negative_float(weightRMSE, \"weightRMSE\")\n",
        "\n",
        "  def fit(self,\n",
        "                  write_results: bool =True,\n",
        "                  path_out: str ='',\n",
        "                  name_append: str ='',\n",
        "                  rLowVar: bool =True,\n",
        "                  applyAbunRel: bool = True,\n",
        "                  allow_transform_high_variation: bool  = True,\n",
        "                  percentage_cols_2_remove: int =1,\n",
        "                  n_Kfold_CV: int=10,\n",
        "                  weightR2: int =1.0,\n",
        "                  weightProbF: float=0.5,\n",
        "                  weightBIC: float=1.0,\n",
        "                  weightRMSE: float=1.5,\n",
        "                  n_max_iter_huber: int=100) -> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    write_results:  bool = False\n",
        "                    Defines if the results will be written. The results include the selected predictors and the metrics for its selection.\n",
        "    path_out: str = \"\"\n",
        "                    Where to write the results\n",
        "    name_append: str = \"\"\n",
        "                    The name to append in the end of the file with the results\n",
        "    rLowVar: bool = True\n",
        "                    Flag to define if it is necessary to apply the removal of predictors with low variance. Set as False if less than 300 predictors.\n",
        "    applyAbunRel: bool = True\n",
        "                    Flag to define if it is necessary to apply the relative abundance transformation. Set as False if the data is already transformed\n",
        "    allow_transform_high_variation: bool = True\n",
        "                    Flag to allow the target transformation in case it has a high variance.\n",
        "    percentage_cols_2_remove: int = 1\n",
        "                    Percentage of the total predictors removed in each iteraction of the RFE. HIGH IMPACT in the final result and computational time.\n",
        "    n_Kfold_CV: int = 10\n",
        "                    Number of folds in the Cross-validation step for the RMSE calculation. HIGH IMPACT in the final result and computational time.\n",
        "    weightR2: float = 1.0\n",
        "                    Weight of the R² metric in the model’s final score\n",
        "    weightProbF: float = 0.5\n",
        "                    Weight of the Probability of the F-test metric in the model’s final score\n",
        "    weightBIC: float = 1.0\n",
        "                    Weight of the BIC metric in the model’s final score\n",
        "    weightRMSE: float = 1.5\n",
        "                    Weight of the RMSE metric in the model’s final score\n",
        "    n_max_iter_huber: int = 100\n",
        "                    Maximum number of iterations of the huber regression. HIGH IMPACT in the final result and computational time.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "                    If any of the parameters is not the correct type or is outside the range\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if type(self.data) == type(None):\n",
        "      print('No data was provided!\\nPlease make sure to provide complete information or use the load_instance(<path_2_instance>) function to load an already created CODARFE model')\n",
        "      return None\n",
        "    print('\\n\\nChecking model parameters...',end=\"\")\n",
        "    self.__check_model_params(write_results,path_out,name_append,rLowVar,applyAbunRel,allow_transform_high_variation,percentage_cols_2_remove,n_Kfold_CV,weightR2,weightProbF,weightBIC,weightRMSE,n_max_iter_huber)\n",
        "    print('OK')\n",
        "\n",
        "    n_cols_2_remove = percentage_cols_2_remove/100\n",
        "    self.__n_max_iter_huber = n_max_iter_huber # Define o numero de iterações utilziado pelo huber\n",
        "\n",
        "    if rLowVar:\n",
        "      #Remove baixa variância\n",
        "      self.__remove_low_var()\n",
        "\n",
        "    if applyAbunRel:\n",
        "      #transforma em abundância relativa\n",
        "      self.__applyAbunRel = True\n",
        "\n",
        "    method = HuberRegressor(epsilon = 2.0,alpha = 0.0003, max_iter = n_max_iter_huber)\n",
        "\n",
        "    # Remove iterativamente atributos enquanto cria vários modelos\n",
        "    resultTable = self.__super_RFE(method,n_cols_2_remove,n_Kfold_CV)\n",
        "\n",
        "    if len(resultTable)>0:\n",
        "      # Calcula pontuação e seleciona o melhor modelo\n",
        "      self.__score_and_selection(resultTable,weightR2,weightProbF,weightBIC,weightRMSE)\n",
        "\n",
        "      self.__define_model(allow_transform_high_variation)\n",
        "\n",
        "      print('\\nModel created!\\n\\n')\n",
        "      print('Results: \\n\\n')\n",
        "      print('R² adj -> ',     self.results['R² adj'])\n",
        "      print('F-statistic -> ',self.results['F-statistic'])\n",
        "      print('BIC -> ',        self.results['BIC'])\n",
        "      print('MSE-CV -> ',     self.results['MSE-CV'])\n",
        "      print('Total of taxa selected -> ',len(self.selected_taxa),'. This value corresponds to ',(len(self.selected_taxa)/self.__totalPredictorsInDatabase)*100,'% of the total.\\n')\n",
        "\n",
        "      if write_results:\n",
        "        self.__write_results(path_out,name_append)\n",
        "\n",
        "    else:\n",
        "      print('The model was not able to generalize your Data.')\n",
        "\n",
        "  def __pairwise_correlation(self,A, B):\n",
        "    am = A - np.mean(A, axis=0, keepdims=True)\n",
        "    bm = B - np.mean(B, axis=0, keepdims=True)\n",
        "    return am.T @ bm /  (np.sqrt(\n",
        "        np.sum(am**2, axis=0,\n",
        "               keepdims=True)).T * np.sqrt(\n",
        "        np.sum(bm**2, axis=0, keepdims=True)))\n",
        "\n",
        "  def __create_correlation_imputer(self):\n",
        "    threshold = 0.6 # Considered as strong correlation\n",
        "    aux = self.__to_CLR(self.data) # Remove composicionalidade usando CLR nos dados originais\n",
        "\n",
        "    for selected in self.selected_taxa: # Para cada taxa selecionada\n",
        "      self.__correlation_list[selected] = [] # Cria instancia para esta taxa selecionada\n",
        "      for taxa in aux.columns: # Verifica correlação com todas as outras\n",
        "        if taxa != selected: # n comparar consigo mesmo\n",
        "          corr = self.__pairwise_correlation(np.array(aux[selected]),np.array(aux[taxa]))# Calcula a correlação de forma rapida\n",
        "          if corr >= threshold: # Somenta adiciona caso seja fortemente correlacionada\n",
        "            self.__correlation_list[selected].append({'taxa':taxa,'corr':corr}) # Adiciona taxa correlacionada\n",
        "      self.__correlation_list[selected].sort(reverse=True,key = lambda x: x['corr']) # Ordena pela correlação\n",
        "\n",
        "  def __Read_new_Data(self,path_to_data):\n",
        "    extension = path_to_data.split('.')[-1]\n",
        "    if extension == 'csv':\n",
        "        data = pd.read_csv(path_to_data,encoding='latin1')\n",
        "        if self.__flag_first_col_as_index_data:\n",
        "          data.set_index(list(data.columns)[0],inplace=True)\n",
        "    elif extension == 'tsv':\n",
        "        data = pd.read_csv(path_to_data,sep='\\t',encoding='latin1')\n",
        "        if self.__flag_first_col_as_index_data:\n",
        "          data.set_index(list(data.columns)[0],inplace=True)\n",
        "    elif extension == 'biom':\n",
        "        table = load_table(path_to_data)\n",
        "        data = table.to_dataframe()\n",
        "    elif extension == 'qza':\n",
        "        output_directory =  '/'.join(path_to_data.split('/')[:-1])+'/QZA_EXTRACT_CODARFE_TEMP/'\n",
        "        # Openning the .qza file as an zip file\n",
        "        with zipfile.ZipFile(path_to_data, 'r') as zip_ref:\n",
        "            # extracting all data to the output directory\n",
        "            zip_ref.extractall(output_directory)\n",
        "        # Getting the biom path file\n",
        "        biompath = output_directory+os.listdir(output_directory)[0]+'/data/'\n",
        "        biompath += [f for f in os.listdir(biompath) if f[-5:]=='.biom'][0]\n",
        "        table = load_table(biompath)# Read the biom file\n",
        "        data = table.to_dataframe() # Tranform it to a pandas dataframe\n",
        "\n",
        "        shutil.rmtree(output_directory) # remove the pathTree created\n",
        "    return data\n",
        "\n",
        "  def predict(self,\n",
        "              path2newdata: str,\n",
        "              applyAbunRel: bool = True,\n",
        "              writeResults: bool = True,\n",
        "              path_out: str = '',\n",
        "              name_append: str = ''\n",
        "              ) -> Optional[Tuple[pd.DataFrame,int]]:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ---------\n",
        "    new : str\n",
        "          The path to the dataframe with new samples for predicting the target variable\n",
        "    applyAbunRel: bool = True\n",
        "          Flag to apply relative abundance transformation\n",
        "    writeResults: bool = False\n",
        "          Flag to write the results\n",
        "    path_out: str = \"\"\n",
        "          Filename of the output. If no filename is provided it will be saved in the same directory as the metadata with the name of 'Prediction.csv'\n",
        "    name_append: str = \"\"\n",
        "          Name to concatenate in the final filename. (Use it to differentiate predictions from the same model)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      Tuple with a Pandas Dataframe and a integer\n",
        "        Pandas Dataframe: Two columns: index and predicts\n",
        "        Integer : The number of predictors that were missing from the new samples (higher the number, higher the error chance; refer to the original paper)\n",
        "\n",
        "      If the new data contains fewer than 25% of the total predictors, no prediction is made and None is returned.\n",
        "\n",
        "    Raises\n",
        "    -------\n",
        "      ModelNotCreatedError:\n",
        "        If the model was not created yet\n",
        "      FileNotFoundError:\n",
        "        If writeResults is True but there is no path_out or path_out does not exists\n",
        "    \"\"\"\n",
        "    if self.__model == None:\n",
        "      raise self.ModelNotCreatedError()\n",
        "\n",
        "    if writeResults and ((path_out != '' and not os.path.exists(path_out)) or path_out == ''):\n",
        "      raise FileNotFoundError('\\nThe path out does not exists or is empty.')\n",
        "\n",
        "    new = self.__Read_new_Data(path2newdata)\n",
        "    newindex = new.index\n",
        "\n",
        "    if self.__correlation_list == {}:\n",
        "      print('\\n\\nCreating correlation list for imputation method. It may take a few minutes depending on the size of the original dataset, but it will be create only once.\\n\\n')\n",
        "      self.__create_correlation_imputer()\n",
        "      print('Correlation list created!\\n\\n')\n",
        "\n",
        "    data2predict = pd.DataFrame() # Cria um dataframe para colocar apenas os dados selecionados\n",
        "    totalNotFound = 0\n",
        "    for selected in self.selected_taxa: # Para cada taxa selecionada\n",
        "      if selected in new.columns: # Caso exista no novo conjunto\n",
        "        data2predict[selected] = new[selected] # Adiciona o valor do novo conjunto no df de previsão\n",
        "      else: # Senão\n",
        "        found = False # Flag que indica se encontrou substitudo\n",
        "        for correlated_2_selected in self.__correlation_list[selected]: # Para cada taxa correlacionada com a que não existe\n",
        "          if correlated_2_selected['taxa'] in new.columns: # Caso encontre um substituto\n",
        "            data2predict[selected] = new[correlated_2_selected['taxa']] # Coloca ele no lugar do que n existe\n",
        "            found = True # Seta flag\n",
        "            break\n",
        "        if not found:\n",
        "          data2predict[selected] = 0 # Caso não encontra retorna zero\n",
        "          print('Warning! Taxa ',selected,' was not found and have no correlations! It may affect the model accuracy')\n",
        "          totalNotFound+=1\n",
        "\n",
        "    if totalNotFound >= len(self.selected_taxa)*0.75:\n",
        "      print('The new samples has less than 25% of selected taxa. The model will not be able to predict it.')\n",
        "      return None,totalNotFound\n",
        "\n",
        "\n",
        "    data2predict = data2predict.fillna(0)\n",
        "\n",
        "    if applyAbunRel:\n",
        "      data2predict = self.__to_abun_rel(data2predict) # Transforma em abundancia relativa\n",
        "\n",
        "    data2predict = self.__to_CLR(data2predict) # Transforma para CLR\n",
        "\n",
        "    resp = self.__model.predict(data2predict)\n",
        "\n",
        "    if self.__sqrt_transform: # Caso o modelo tenha sido treinado nos dados log transformados\n",
        "      resp = self.__calc_inverse_sqrt_redimension(resp)#,totalNotFound # Retorna os valores restaurados ao original\n",
        "    if self.__transform:\n",
        "      resp = self.__calc_inverse_redimension(resp) # Retorna os valores restaurados ao original\n",
        "\n",
        "    if writeResults:\n",
        "\n",
        "      if path_out != '':\n",
        "        if path_out[-1]!= '/':\n",
        "          path_out+='/Prediction'\n",
        "      else:\n",
        "        path_out = '/'.join(self.__path_to_metadata.split('/')[:-1])+'/Prediction'\n",
        "      if name_append != '':\n",
        "        name_append = '_'+name_append\n",
        "      filename = path_out+name_append+'.csv'\n",
        "      pd.DataFrame(data = resp,columns = ['Prediction'],index=newindex).to_csv(filename)\n",
        "\n",
        "    return resp,totalNotFound\n",
        "\n",
        "  def plot_correlation(self,saveImg: bool=False,path_out: str='',name_append: str='') -> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ---------\n",
        "    saveImg:  bool = False\n",
        "              Flag that defines if the img will be saved\n",
        "    path_out: str = \"\"\n",
        "              The path to the folder where the img will be saved\n",
        "    name_append: str = \"\"\n",
        "              The name to append in the end of the img name (Correlation_<name_append>)\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ModelNotCreatedError:\n",
        "              if the CODARFE.CreateModel was not run yet\n",
        "    FileNotFoundError:\n",
        "              If the path_out does not exists\n",
        "    \"\"\"\n",
        "    if self.__model == None:\n",
        "      raise self.ModelNotCreatedError()\n",
        "\n",
        "    if path_out != '' and not os.path.exists(path_out):\n",
        "      raise FileNotFoundError(\"\\nThe path out does not exists.\\nPlease try again with the correct path or let it blank to write in the same path as the metadata\")\n",
        "\n",
        "    # Build a rectangle in axes coords\n",
        "    left, width = .15, .75\n",
        "    bottom, height = .15, .75\n",
        "    right = left + width\n",
        "    top = bottom + height\n",
        "    y = self.target\n",
        "    X = self.data[self.selected_taxa]\n",
        "    \n",
        "    if self.__applyAbunRel:\n",
        "      X = self.__to_abun_rel(X)\n",
        "\n",
        "    X = self.__to_CLR(X)\n",
        "    pred = self.__model.predict(X)\n",
        "\n",
        "    if self.__sqrt_transform: # Caso tenha aprendido com valores transformados\n",
        "      pred = self.__calc_inverse_sqrt_redimension(pred) # Destransforma-os\n",
        "    if self.__transform:\n",
        "      pred = self.__calc_inverse_redimension(pred) # Destransforma-os\n",
        "\n",
        "    # Initialize the matplotlib figure\n",
        "    plt.figure()\n",
        "    plt.clf()\n",
        "    ax = plt.gca()\n",
        "\n",
        "    corr, what = pearsonr(y, pred)\n",
        "\n",
        "    #Plota os pontos previsto por esperado\n",
        "    plt.plot(pred, y, 'o')\n",
        "\n",
        "    # calcula o slop e intercept para uma regressão linear (plot da linha)\n",
        "    m, b = np.polyfit(pred, y, 1)\n",
        "\n",
        "    #Adiciona a linha no plot\n",
        "    plt.plot(pred, m*np.array(pred)+b)\n",
        "    shiftX = 0.2 * max(pred)\n",
        "    shiftY = 0.1 * max(y)\n",
        "\n",
        "    ax.text(left, top, 'R = '+str(round(corr,2))+', p < '+str(round(what,2)),\n",
        "          horizontalalignment='center',\n",
        "          verticalalignment='center',\n",
        "          transform=ax.transAxes)\n",
        "\n",
        "    if path_out != '':\n",
        "      if path_out[-1]!= '/':\n",
        "        path_out+='/'\n",
        "    else:\n",
        "      path_out = '/'.join(self.__path_to_metadata.split('/')[:-1])+'/'\n",
        "    # adiciona '_' caso n tenha\n",
        "    if name_append != '':\n",
        "      if name_append[0]!= '_':\n",
        "        name_append = 'Correlation_'+name_append\n",
        "    else:\n",
        "      name_append = 'Correlation'\n",
        "    filename = path_out+name_append+'.png'\n",
        "\n",
        "    print('\\nSaving the image in ',filename)\n",
        "    plt.savefig(filename, dpi=600, bbox_inches='tight')\n",
        "\n",
        "  def __checkHoldOutParams(self,n_repetitions,test_size,path_out,name_append):\n",
        "    self.__check_integer(n_repetitions,\"n_repetitions\")\n",
        "    self.__check_integer_range(n_repetitions,\"n_repetitions\",2,1000)\n",
        "    self.__check_integer(test_size,\"test_size\")\n",
        "    self.__check_integer_range(test_size,\"test_size\",1,99)\n",
        "    if path_out != '' and not os.path.exists(path_out):\n",
        "      raise FileNotFoundError(\"\\nThe path out does not exists.\\nPlease try again with the correct path or let it blank to write in the same path as the metadata\")\n",
        "\n",
        "  def plot_holdOut_validation(self,\n",
        "                              n_repetitions: int = 100,\n",
        "                              test_size: int = 20,\n",
        "                              saveImg: str = False,\n",
        "                              path_out: str = '',\n",
        "                              name_append: str = '') -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ---------\n",
        "    n_repetitions:  int = 100\n",
        "                    Defines the number of repetitions (dots in the plot)\n",
        "    test_size: int = 20\n",
        "                    Defines the size of the hold-out samples\n",
        "    saveImg:  bool = False\n",
        "                    Flag that defines if the img will be saved\n",
        "    path_out: str = \"\"\n",
        "                    The path to the folder where the img will be saved\n",
        "    name_append: str = \"\"\n",
        "                    The name to append in the end of the img name (HoldOut_Validation_<name_append>)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "              If any of the parameters is not the correct type or is outside the range\n",
        "    ModelNotCreatedError\n",
        "              if the CODARFE.CreateModel was not run yet\n",
        "    FileNotFoundError:\n",
        "              If the path_out does not exists\n",
        "    \"\"\"\n",
        "    if self.__model == None:\n",
        "      raise self.ModelNotCreatedError()\n",
        "\n",
        "    self.__checkHoldOutParams(n_repetitions,test_size,path_out,name_append)\n",
        "\n",
        "    test_size = test_size/100\n",
        "    method = RandomForestRegressor(n_estimators = 160, criterion = 'poisson',random_state=42)\n",
        "    X = self.data[self.selected_taxa]\n",
        "    if self.__applyAbunRel:\n",
        "      X = self.__to_abun_rel(X)\n",
        "    X = self.__to_CLR(X)\n",
        "    y = self.target\n",
        "    maes = []\n",
        "    out = display(self.__progress(0, 100), display_id=True)\n",
        "    for i in range(n_repetitions):\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size) # divide em treino e teste\n",
        "\n",
        "      if self.__sqrt_transform: # Caso tenha aprendido originalmente com valores transformados\n",
        "        method.fit(X_train,self.__calc_new_sqrt_redimension(y_train)) # Re-treina com os valores transformados\n",
        "        y_pred = method.predict(X_test) # Realiza a predição\n",
        "        y_pred = self.__calc_inverse_sqrt_redimension(y_pred) # Destransforma-os\n",
        "      elif self.__transform:\n",
        "        method.fit(X_train,self.__calc_new_redimension(y_train))\n",
        "        y_pred = method.predict(X_test)\n",
        "        y_pred = self.__calc_inverse_redimension(y_pred)\n",
        "      else:\n",
        "        method.fit(X_train,y_train)\n",
        "        y_pred = method.predict(X_test)\n",
        "\n",
        "      # Calculo do MAE\n",
        "      tt = 0\n",
        "      for ii in range(len(y_pred)):\n",
        "        tt+=abs(y_test.iloc[ii]-y_pred[ii])\n",
        "      maes.append(tt/len(y_pred))\n",
        "      out.update(self.__progress(int((i/n_repetitions)*100), 100))\n",
        "\n",
        "    sns.set_theme(style=\"ticks\")\n",
        "    out.update(self.__progress(100, 100))\n",
        "\n",
        "    # Cria a figura zerada\n",
        "    plt.figure()\n",
        "    f, ax = plt.subplots(figsize=(7, 6))\n",
        "\n",
        "    # Plota o boxplot\n",
        "    sns.boxplot(x=[1]*len(maes),\n",
        "                y=maes,\n",
        "                whis=[0, 100],\n",
        "                width=.6,\n",
        "                palette=\"vlag\")\n",
        "\n",
        "    # Adiciona os pontos sobre o boxplot\n",
        "    sns.stripplot(x=[1]*len(maes),\n",
        "                  y=maes,\n",
        "                  size=4,\n",
        "                  color=\".3\",\n",
        "                  linewidth=0)\n",
        "\n",
        "    # Tweak the visual presentation\n",
        "    ax.xaxis.grid(True)\n",
        "    ax.set(ylabel=\"\")\n",
        "    sns.despine(trim=True, left=True)\n",
        "\n",
        "    trainSize = int((1-test_size) *100)\n",
        "    testSize = int(test_size*100)\n",
        "    ax.set_title('Hold-out Validation ('+str(trainSize)+'-'+str(testSize)+') '+str(n_repetitions)+' repetitions',fontweight='bold')\n",
        "    ax.set_ylabel('Mean Absolute Error')\n",
        "    #ax.set_xlabel(target+' MAE')\n",
        "\n",
        "    if path_out != '':\n",
        "      if path_out[-1]!= '/':\n",
        "        path_out+='/'\n",
        "    else:\n",
        "      path_out = '/'.join(self.__path_to_metadata.split('/')[:-1])+'/'\n",
        "    # adiciona '_' caso n tenha\n",
        "    if name_append != '':\n",
        "      if name_append[0]!= '_':\n",
        "        name_append = 'HoldOut_Validation_'+name_append\n",
        "    else:\n",
        "      name_append = 'HoldOut_Validation'\n",
        "    filename = path_out+name_append+'.png'\n",
        "\n",
        "    print('\\nSaving the image in ',filename)\n",
        "    plt.savefig(filename, dpi=600, bbox_inches='tight')\n",
        "\n",
        "\n",
        "  def plot_relevant_predictors(self,\n",
        "                               n_max_features: int = 100,\n",
        "                               saveImg: bool = False,\n",
        "                               path_out: str = '',\n",
        "                               name_append: str = '') -> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ---------\n",
        "    n_max_features: int = 100\n",
        "                    Defines the maximum number of features/predictors to be displayed (bars in the plot)\n",
        "    saveImg:  bool = False\n",
        "                    Flag that defines if the img will be saved\n",
        "    path_out: str = \"\"\n",
        "                    The path to the folder where the img will be saved\n",
        "    name_append: str = \"\"\n",
        "                    The name to append in the end of the img name (HoldOut_Validation_<name_append>)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "              If any of the parameters is not the correct type or is outside the range\n",
        "    ModelNotCreatedError\n",
        "              if the CODARFE.CreateModel was not run yet\n",
        "    FileNotFoundError:\n",
        "              If the path_out does not exists\n",
        "    \"\"\"\n",
        "\n",
        "    if self.__model == None:\n",
        "      raise self.ModelNotCreatedError()\n",
        "    self.__check_integer(n_max_features,\"n_max_features\")\n",
        "    self.__check_integer_range(n_max_features,\"n_max_features\",2,1000)\n",
        "\n",
        "    if path_out != '' and not os.path.exists(path_out):\n",
        "      raise FileNotFoundError(\"\\nThe path out does not exists.\\nPlease try again with the correct path or let it blank to write in the same path as the metadata\")\n",
        "\n",
        "\n",
        "    method = HuberRegressor(epsilon = 2.0,alpha = 0.0003, max_iter = self.__n_max_iter_huber)\n",
        "    X = self.data[self.selected_taxa]\n",
        "    if self.__applyAbunRel:\n",
        "      X = self.__to_abun_rel(X)\n",
        "    X = self.__to_CLR(X)\n",
        "    # y = self.target\n",
        "    if self.__sqrt_transform:\n",
        "      y = np.array(self.__calc_new_sqrt_redimension(self.target))\n",
        "    elif self.__transform:\n",
        "      y = np.array(self.__calc_new_redimension(self.target))\n",
        "    else:\n",
        "      y = self.target\n",
        "    resp = method.fit(X,y)\n",
        "\n",
        "    dfaux = pd.DataFrame(data={'features':resp.feature_names_in_,'coefs':resp.coef_})\n",
        "    dfaux.sort_values(by='coefs',ascending=False,inplace=True,ignore_index=True)\n",
        "\n",
        "    if len(dfaux) > n_max_features:\n",
        "      half = int(n_max_features/2)\n",
        "      totpos = len(dfaux.coefs[dfaux.coefs>0])\n",
        "      totneg = len(dfaux.coefs[dfaux.coefs<0])\n",
        "\n",
        "      if totpos < half:\n",
        "        totneg = half+half-totpos\n",
        "      elif totneg < half:\n",
        "        totpos = half+half-totneg\n",
        "      else:\n",
        "        totpos = half\n",
        "        totneg = half\n",
        "\n",
        "      dfaux = dfaux[dfaux.index.isin([i for i in range(0,totpos)] + [i for i in range(len(dfaux)-totneg,len(dfaux))])]\n",
        "    plt.figure()\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "    # Initialize the matplotlib figure\n",
        "    f, ax = plt.subplots(figsize=(6, 15))#figsize=(6, 15)\n",
        "\n",
        "    colors = ['b' if x > 0 else 'r' for x in dfaux.coefs]\n",
        "    # Plot the total crashes\n",
        "    sns.set_color_codes(\"pastel\")\n",
        "    sns.barplot(x=\"coefs\",\n",
        "                y=\"features\",\n",
        "                data=dfaux,\n",
        "                palette=colors,\n",
        "                )\n",
        "\n",
        "    ax.set_title('Strength of relevant predictors',fontweight='bold')\n",
        "    ax.set(ylabel=\"Predictor name\",\n",
        "          xlabel=\"Coefficient weight\")\n",
        "    sns.despine(left=True, bottom=True)\n",
        "\n",
        "    if path_out != '':\n",
        "      if path_out[-1]!= '/':\n",
        "        path_out+='/'\n",
        "    else:\n",
        "      path_out = '/'.join(self.__path_to_metadata.split('/')[:-1])+'/'\n",
        "    # adiciona '_' caso n tenha\n",
        "    if name_append != '':\n",
        "      if name_append[0]!= '_':\n",
        "        name_append = 'Relevant_Predictors_'+name_append\n",
        "    else:\n",
        "      name_append = 'Relevant_Predictors_'\n",
        "    filename = path_out+name_append+'.png'\n",
        "\n",
        "\n",
        "    print('\\nSaving the image in ',filename)\n",
        "    plt.savefig(filename, dpi=600, bbox_inches='tight')\n",
        "\n",
        "\n",
        "  # HEAT MAP (ps: eu n lembro de porra nenhuma de como eu criei isso... melhor n tentar otimizar nada)\n",
        "  def __neatMapLinkage(self,selected_features):\n",
        "    w = ca(selected_features)\n",
        "    pc1 = w.features['CA1']\n",
        "    pc2 = w.features['CA2']\n",
        "\n",
        "    xc = np.mean(pc1)\n",
        "    yc = np.mean(pc2)\n",
        "    theta = []\n",
        "    for i in range(len(pc1)):\n",
        "      theta.append(math.atan2(pc2[i] - yc, pc1[i] - xc ))\n",
        "    order = [index for index, element in sorted(enumerate(theta), key=operator.itemgetter(1))]\n",
        "    names = [selected_features.columns[i] for i in order]\n",
        "    return names\n",
        "\n",
        "  def __heatmap(self,data, row_labels, col_labels, ax=None,cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
        "\n",
        "    if not ax:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    # Plot the heatmap\n",
        "    im = ax.imshow(data, **kwargs)\n",
        "\n",
        "    # Create colorbar\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    cbar = ax.figure.colorbar(im, ax=ax,cax=cax, **cbar_kw)#im\n",
        "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
        "\n",
        "    # Show all ticks and label them with the respective list entries.\n",
        "    ax.set_xticklabels(labels=col_labels)\n",
        "    ax.set_yticklabels(labels=row_labels)\n",
        "    ax.set_xticks(np.arange(len(col_labels)))\n",
        "    ax.set_yticks(np.arange(len(row_labels)))\n",
        "    # Let the horizontal axes labeling appear on top.\n",
        "    ax.tick_params(top=True, bottom=False,\n",
        "                    labeltop=True, labelbottom=False)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=-90, ha=\"right\",\n",
        "              rotation_mode=\"anchor\")\n",
        "\n",
        "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
        "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
        "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
        "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "    return im, cbar\n",
        "\n",
        "  def plot_heatmap(self,\n",
        "                   saveImg: bool=False,\n",
        "                   path_out: str='',\n",
        "                   name_append: str=''\n",
        "                   ) -> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ---------\n",
        "    saveImg:  bool = False\n",
        "              Flag that defines if the img will be saved\n",
        "    path_out: str = \"\"\n",
        "              The path to the folder where the img will be saved\n",
        "    name_append: str = \"\"\n",
        "              The name to append in the end of the img name (HeatMap_<name_append>)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ModelNotCreatedError\n",
        "              if the CODARFE.CreateModel was not run yet\n",
        "    FileNotFoundError:\n",
        "              If the path_out does not exists\n",
        "    \"\"\"\n",
        "\n",
        "    if self.__model == None:\n",
        "      raise self.ModelNotCreatedError()\n",
        "\n",
        "    if path_out != '' and not os.path.exists(path_out):\n",
        "      raise FileNotFoundError(\"\\nThe path out does not exists.\\nPlease try again with the correct path or let it blank to write in the same path as the metadata\")\n",
        "\n",
        "    # Pega o dataframe original porem apenas o que foi selecioando\n",
        "    selected_features = self.data[self.selected_taxa]\n",
        "    if self.__applyAbunRel:\n",
        "      selected_features = self.__to_abun_rel(selected_features)\n",
        "    # Clusterizando bacterias\n",
        "    y = self.target\n",
        "\n",
        "    ###### Aqui clusteriza por CA ############\n",
        "    leaf_names = self.__neatMapLinkage(selected_features)\n",
        "    ##########################################\n",
        "    clustered_df = pd.DataFrame()\n",
        "\n",
        "    for name in leaf_names:\n",
        "      clustered_df[name] = selected_features[name]\n",
        "    clustered_df['Target'] = y\n",
        "    selected_features = clustered_df\n",
        "\n",
        "    # Ordenando bacterias por variável alvo\n",
        "    selected_features_t = selected_features.T\n",
        "    sorted_t = selected_features_t.sort_values(by='Target',axis=1,ascending=False)\n",
        "    y = list(sorted_t.iloc[-1])\n",
        "\n",
        "    # Separando os dados para o plot\n",
        "    bac_counts = sorted_t.drop('Target',axis=0).replace(0,0.5).values\n",
        "\n",
        "    bacs = list(sorted_t.drop('Target',axis=0).index[:])\n",
        "\n",
        "    # Aplica o CLR\n",
        "    bac_clr = clr(bac_counts+0.001)\n",
        "    vmin = min(bac_clr.flatten())\n",
        "    vmax = max(bac_clr.flatten())\n",
        "    norm = colors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
        "\n",
        "    Largura = int(len(y)*0.2)\n",
        "    Altura  = int(len(bac_counts)*0.2)\n",
        "    if Largura < 15:\n",
        "      Largura = 15\n",
        "    if Altura < 20:\n",
        "      Altura = 20\n",
        "    if Largura > 150:\n",
        "      Largura = 150\n",
        "    if Altura > 200:\n",
        "      Altura = 200\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(figsize=(Largura,Altura))\n",
        "\n",
        "    im, cbar = self.__heatmap(bac_clr, bacs, y, ax=ax, cmap=\"RdYlBu\",norm = norm, cbarlabel=\"Center-Log-Ratio\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "\n",
        "    if path_out != '':\n",
        "      if path_out[-1]!= '/':\n",
        "        path_out+='/'\n",
        "    else:\n",
        "      path_out = '/'.join(self.__path_to_metadata.split('/')[:-1])+'/'\n",
        "    # adiciona '_' caso n tenha\n",
        "    if name_append != '':\n",
        "      if name_append[0]!= '_':\n",
        "        name_append = 'HeatMap_'+name_append\n",
        "    else:\n",
        "      name_append = 'HeatMap'\n",
        "    filename = path_out+name_append+'.png'\n",
        "\n",
        "\n",
        "    print('\\nSaving the image in ',filename)\n",
        "    plt.savefig(filename, dpi=600, bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qiFMUzFTPUN"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oys1XFOoj3FZ"
      },
      "outputs": [],
      "source": [
        "basepath = 'base/path/out'\n",
        "coda = CODARFE(path2Data = 'path/2/your/data',\n",
        "               path2MetaData = 'path/2/your/meta/data',\n",
        "               metaData_Target = 'target')\n",
        "#or\n",
        "#coda.Load_Instance() # MUST provide path to '*.foda' file\n",
        "coda.CreateModel(write_results=True,\n",
        "                 path_out = basepath,\n",
        "                 name_append ='',\n",
        "                 rLowVar=True,\n",
        "                 applyAbunRel= True,\n",
        "                 allow_transform_high_variation = True,\n",
        "                 percentage_cols_2_remove =1,\n",
        "                 n_Kfold_CV=10,\n",
        "                 weightR2 =1.0,\n",
        "                 weightProbF=0.5,\n",
        "                 weightBIC=1.0,\n",
        "                 weightRMSE=1.5,\n",
        "                 n_max_iter_huber=100)\n",
        "\n",
        "coda.Save_Instance(basepath) # default name is CODARFE_RESULTS\n",
        "coda.Predict('path/2/your/new/samples') # MUST provide the path to the new samples\n",
        "\n",
        "coda.Plot_Correlation(basepath) # Displays and saves the predicted x expected correction plot of the triene\n",
        "coda.Plot_HoldOut_Validation(basepath) # Performs hold out validation 25 times and displays the MAE boxplot\n",
        "coda.Plot_Relevant_Predictors(basepath) # Display and save selected predictors with their correlation strength\n",
        "coda.Plot_Heatmap(basepath) # Display and save the heatmap\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
