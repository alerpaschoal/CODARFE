{"cells":[{"cell_type":"markdown","metadata":{"id":"Zxh4aammSa1P"},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKW4BXdiSdln"},"outputs":[],"source":["# Installing requiriments\n","!pip install biom-format\n","!pip install scikit-bio\n","\n","# Imports\n","from biom import load_table\n","import zipfile\n","import shutil\n","import os\n","from skbio.stats.composition import clr\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as stats\n","import math\n","import operator\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.ensemble import RandomForestRegressor\n","import sys\n","from sklearn.linear_model import HuberRegressor\n","from time import perf_counter\n","from multiprocessing import Process\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","import pickle as pk\n","import matplotlib.pyplot as plt\n","from scipy.stats import pearsonr\n","import seaborn as sns\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","import matplotlib.colors as colors\n","from skbio.stats.ordination import ca\n","import warnings\n","from IPython.display import HTML, display\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"OV_Fts_HSd_9"},"source":["# CODARFE Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SU8ax4VeR_cx"},"outputs":[],"source":["class CODARFE():\n","  def __init__(self,\n","               path2Data = None,\n","               path2MetaData = None,\n","               metaData_Target=None\n","               ):\n","    self.__path2MetaData = path2MetaData\n","    if path2Data != None and path2MetaData != None and metaData_Target != None:\n","      print(\"Loading data... It may take some minutes depending on the size of the data\")\n","      self.data, self.target = self.__Read_Data(path2Data,path2MetaData,metaData_Target)\n","      self.__totalPredictorsInDatabase = len(self.data.columns)\n","    else:\n","      self.data = None\n","      self.target = None\n","      # print('No complete data provided. Please use the function Load_Instance(<path_2_instance>) to load an already created CODARFE model.')\n","\n","    self.__log_transform = None\n","    self.__min_target_log_transformed = None\n","    self.__max_target_log_transformed = None\n","    self.results = None\n","    self.score_best_model = None\n","    self.selected_taxa = None\n","    self.__model = None\n","    self.__n_max_iter_huber = None\n","\n","    self.__correlation_list = {}\n","\n","  def __Read_Data(self,path2Data,path2metadata,target_column_name):\n","    if not os.path.exists(path2Data):\n","      print('The Data file does not exists!')\n","      sys.exit(1)\n","    if not os.path.exists(path2metadata):\n","      print('The Metadata file does not exists!')\n","      sys.exit(1)\n","    extension = path2Data.split('.')[-1]\n","    if extension == 'csv':\n","        data = pd.read_csv(path2Data,encoding='latin1')\n","        data.set_index(list(data.columns)[0],inplace=True)\n","    elif extension == 'tsv':\n","        data = pd.read_csv(path2Data,sep='\\t',encoding='latin1')\n","        data.set_index(list(data.columns)[0],inplace=True)\n","    elif extension == 'biom':\n","        table = load_table(path2Data)\n","        data = table.to_dataframe()\n","    elif extension == 'qza':\n","        output_directory =  '/'.join(path2Data.split('/')[:-1])+'/QZA_EXTRACT_CODARFE_TEMP/'\n","        # Openning the .qza file as an zip file\n","        with zipfile.ZipFile(path2Data, 'r') as zip_ref:\n","            # extracting all data to the output directory\n","            zip_ref.extractall(output_directory)\n","        # Getting the biom path file\n","        biompath = output_directory+os.listdir(output_directory)[0]+'/data/'\n","        biompath += [f for f in os.listdir(biompath) if f[-5:]=='.biom'][0]\n","        table = load_table(biompath)# Read the biom file\n","        data = table.to_dataframe() # Tranform it to a pandas dataframe\n","\n","        shutil.rmtree(output_directory) # remove the pathTree created\n","\n","    extension = path2metadata.split('.')[-1]\n","    if extension == 'csv':\n","        metadata = pd.read_csv(path2metadata,encoding='latin1')\n","        metadata.set_index(list(metadata.columns)[0],inplace=True)\n","    elif extension == 'tsv':\n","        metadata = pd.read_csv(path2metadata,sep='\\t',encoding='latin1')\n","        metadata.set_index(list(metadata.columns)[0],inplace=True)\n","\n","    totTotal = len(metadata)\n","    if target_column_name not in metadata.columns:\n","      print(\"The Target is not present in the metadata table!\")\n","      sys.exit(1)\n","    totNotNa = metadata[target_column_name].isna().sum()\n","    metadata.dropna(subset=[target_column_name],inplace=True)\n","\n","\n","    data = data.loc[metadata.index]\n","    y = metadata[target_column_name]\n","\n","    if len(data) == 0:\n","      print('There is no correspondence between the ids of the predictors and the metadata.\\nMake sure the column corresponding to the identifiers is first.')\n","      sys.exit(1)\n","    print('Total samples with the target variable: ',totTotal-totNotNa,'/',totTotal)\n","\n","    return data,y\n","\n","  def Save_Instance(self,path_out = \"\",name_append=''):\n","\n","    if type(self.data) == type(None):\n","      print('Nothing to save.\\n\\nPlease when creating the CODARFE instance provide full data information: <path_2_Data> , <path_2_MetaData> and <metaData_Target>')\n","      return\n","\n","    if path_out != '':\n","      if path_out[-1]!= '/':\n","        path_out+='/'\n","    else:\n","      path_out = '/'.join(self.__path2MetaData.split('/')[:-1])+'/'\n","    # adiciona '_' caso n tenha\n","    if name_append != '':\n","      if name_append[0]!= '_':\n","        name_append = 'CODARFE_MODEL_'+name_append\n","    else:\n","      name_append = 'CODARFE_MODEL'\n","    filename = path_out+name_append+'.foda'\n","\n","    obj = {'data':self.data,\n","           'target':self.target,\n","           'log_transform': self.__log_transform,\n","           'min_target_log_transformed': self.__min_target_log_transformed,\n","           'max_target_log_transformed': self.__max_target_log_transformed,\n","           'results': self.results,\n","           'score_best_model': self.score_best_model,\n","           'selected_taxa': self.selected_taxa,\n","           'model': self.__model,\n","           'n_max_iter_huber': self.__n_max_iter_huber,\n","           'correlation_list':self.__correlation_list}\n","\n","\n","    with open(filename,'wb') as f:\n","      pk.dump(obj,f)\n","\n","    print('\\n\\nInstance saved at ',filename,'\\n\\n')\n","\n","  def Load_Instance(self,path2instance):\n","    with open(path2instance,'rb') as f:\n","      obj = pk.load(f)\n","\n","    self.data = obj['data']\n","    self.target = obj['target']\n","    self.__log_transform = obj['log_transform']\n","    self.__min_target_log_transformed = obj['min_target_log_transformed']\n","    self.__max_target_log_transformed = obj['max_target_log_transformed']\n","    self.results = obj['results']\n","    self.score_best_model = obj['score_best_model']\n","    self.selected_taxa = obj['selected_taxa']\n","    self.__model = obj['model']\n","    self.__n_max_iter_huber = obj['n_max_iter_huber']\n","    self.__correlation_list = obj['correlation_list']\n","\n","    print('\\n\\nInstance restored successfully!\\n\\n')\n","\n","\n","  def __removeLowVar(self):\n","    aux = self.data.copy()\n","    cols = aux.columns\n","    selector = VarianceThreshold(aux.var(axis=1).mean()/8)#8\n","    aux = selector.fit(aux)\n","    not_to_drop=list(cols[selector.get_support()])\n","    totRemoved = len(self.data.columns) - len(not_to_drop)\n","    print('\\nA total of ',totRemoved,' taxa were removed due to very low variance\\n')\n","    self.data =  self.data[not_to_drop]\n","\n","  def __toAbunRel(self,data):\n","    return data.apply(lambda x: x/x.sum() if x.sum()!=0 else x,axis=1)\n","\n","  def __calc_new_redimension(self,target):\n","    target = np.log2(target)# 1° transforma pra log2\n","\n","    minimo = min(target)\n","    maximo = max(target)\n","    if self.__min_target_log_transformed == None or self.__max_target_log_transformed == None:\n","      self.__min_target_log_transformed = minimo\n","      self.__max_target_log_transformed = maximo\n","\n","    new_min = 0  # novo valor mínimo desejado\n","    new_max = 100  # novo valor máximo desejado\n","\n","    numeros_redimensionados = [(x - minimo) / (maximo - minimo) * (new_max - new_min) + new_min for x in target]\n","    return numeros_redimensionados\n","\n","  def __calc_inverse_redimension(self,predictions):\n","\n","    new_min = 0  # novo valor mínimo usado na transformação\n","    new_max = 100  # novo valor máximo usado na transformação\n","\n","    minimo = self.__min_target_log_transformed\n","    maximo = self.__max_target_log_transformed\n","\n","    numeros_restaurados = [(x - new_min) / (new_max - new_min) * (maximo - minimo) + minimo for x in predictions]\n","    numeros_restaurados_log_inverse = np.power(2, numeros_restaurados)\n","    return numeros_restaurados_log_inverse\n","\n","  def __toCLR(self,df): # Transform to CLr\n","    aux = df.copy()\n","    cols = aux.columns\n","    aux+=0.0001 # Pseudo count\n","    aux = clr(aux)\n","    aux = pd.DataFrame(data=aux,columns=cols)\n","    return aux\n","\n","  def __calc_mse_model_centeredv2(self,pred,y,df_model): #pred é o valor predito  # df_model é o número de variáveis\n","    mean = np.mean(y)\n","    ssr = sum([(yy-pp)**2 for yy,pp in zip(y,pred)])\n","    centered_tss = sum([(aux-mean)**2 for aux in y])\n","    ess = centered_tss - ssr\n","    return ess/df_model\n","\n","  def __calc_mse_resid(self,pred,y,df_resid):\n","    residuos = [yy-pp for yy,pp in zip(y,pred)]\n","    return sum([aux**2 for aux in residuos])/df_resid\n","\n","  def __calc_f_prob_centeredv2(self,pred,y,X):\n","    df_model = max(1,len(X.iloc[0]) - 1)\n","    df_resid = max(1,len(X) - df_model -1 )\n","    mse_model = self.__calc_mse_model_centeredv2(pred,y,df_model)\n","    mse_resid = self.__calc_mse_resid(pred,y,df_resid)\n","    fstatistic = mse_model/mse_resid\n","    return stats.f.sf(fstatistic, df_model, df_resid)\n","\n","  def __calc_r_squared(self,pred,y,method = 'centered'):\n","    ssr = sum([(yy-pp)**2 for yy,pp in zip(y,pred)])\n","    mean = np.mean(y)\n","    center_tss = np.sum((y - mean)**2)\n","    uncentered_tss = sum([(aux)**2 for aux in y])\n","    if method == 'centered':\n","      return 1 - ssr/center_tss\n","    else:\n","      return 1 - ssr/uncentered_tss\n","\n","  def __calc_rsquared_adj(self,X,r2):\n","    return 1 - (1-r2) * (X.shape[0]-1)/(X.shape[0]-X.shape[1]-1)\n","\n","  def __calc_llf(self,pred,y,X):\n","    nobs2 = len(X) / 2.0\n","    nobs = float(len(X))\n","    ssr = sum([(yy-pp)**2 for yy,pp in zip(y,pred)])\n","    llf = -nobs2*np.log(2*np.pi) - nobs2*np.log(ssr / nobs) - nobs2\n","    return llf\n","\n","\n","  def __calc_bic(self,pred,y,X):\n","    llf = self.__calc_llf(pred,y,X)\n","    nobs = float(len(X))\n","    k_params = len(X.iloc[0])\n","    bic = -2*llf + np.log(nobs) * k_params\n","    return round(bic)\n","\n","  def __progress(self,value, max=100):\n","    return HTML(\"\"\"\n","        <progress\n","            value='{value}'\n","            max='{max}',\n","            style='width: 100%'\n","        >\n","            {value}\n","        </progress>\n","    \"\"\".format(value=value, max=max))\n","\n","  def __superRFE(self,method,n_cols_2_remove,n_Kfold_CV):\n","    # Define o total de atributos a serem removidos por rodada\n","    n_cols_2_remove = max([int(len(self.data.columns)*n_cols_2_remove),1])\n","\n","    # Define X inicial como descritores\n","    X_train = self.data\n","    tot2display = len(list(X_train.columns))\n","    # Define variável alvo\n","    if self.__log_transform:\n","      y_train = np.array(self.__calc_new_redimension(self.target))\n","    else:\n","      y_train = self.target\n","\n","    # Inicializa tabela de resultados\n","    resultTable = pd.DataFrame(columns=['Atributos','R² adj','F-statistic','BIC','MSE-CV'])\n","    percentagedisplay = round(100 - (len(list(X_train.columns))/tot2display)*100)\n","    out = display(self.__progress(0, 100), display_id=True)\n","    while len(X_train.columns) - n_cols_2_remove > n_cols_2_remove or len(X_train.columns) > 1:\n","      X = self.__toCLR(X_train)\n","\n","      method.fit(X,y_train)\n","\n","      pred = method.predict(X)\n","\n","      Fprob = self.__calc_f_prob_centeredv2(pred,y_train,X)\n","\n","      r2 = self.__calc_r_squared(pred,y_train)\n","      r2adj = self.__calc_rsquared_adj(X,r2)\n","\n","      BIC = self.__calc_bic(pred,y_train,X)\n","\n","      msecv_results = []\n","      #Adicionando etapa de validação cruzada\n","      kf = KFold(n_splits=n_Kfold_CV,shuffle=True,random_state=42)\n","\n","      for train, test in kf.split(X):\n","\n","        model = method.fit(X.iloc[train],y_train[train])\n","\n","        predcv = model.predict(X.iloc[test])\n","\n","        msecv_results.append(np.mean([(t-p)**2 for t,p in zip(y_train[test],predcv)])**(1/2))\n","\n","      msecv = np.mean(msecv_results)\n","\n","      # caso consiga calcular um p-valor\n","      if not math.isnan(Fprob) and r2adj < 1.0:\n","        # Cria linha com: Atributos separados por ';', R² ajustado, estatistica F e estatistica BIC\n","        newline = pd.DataFrame.from_records([{'Atributos':'@'.join(list(X.columns)),'R² adj':float(r2adj),'F-statistic':float(Fprob),'BIC':float(BIC),'MSE-CV':float(msecv)}])\n","\n","        # Adiciona linha na tabela de resultado\n","        resultTable = pd.concat([resultTable,newline])\n","\n","      # Cria tabela de dados dos atributos\n","      atr = pd.DataFrame(data = [[xx,w] for xx,w in zip(X.columns,method.coef_)],columns = ['Atributos','coef'])\n","\n","      # Transforma coluna de coeficiente p/ float\n","      atr = atr.astype({'coef':float})\n","\n","      # Remove colunas com coeficientes iguais ou menores que 0 / Acelera o rfe\n","      atr = atr[atr.coef != 0]# Remove zeros\n","      atr.coef = atr.coef.abs()#Transforma em abs para remover apenas os X% mais perto de zero\n","\n","      # Ordena de forma decrescente pelo coeficiente\n","      atr = atr.sort_values(by=['coef'],ascending=False)\n","\n","      # Cria lista de atributos selecionados\n","      atributos = list(atr.Atributos)\n","\n","      # Remove espaços em brancos inseridos pela tabela de atributos\n","      atributos = [x.strip() for x in atributos]\n","\n","      # Remove n_cols_2_remove menos relevantes\n","      atributos = atributos[:-n_cols_2_remove]\n","\n","      # Remove atributos n selecionados\n","      X_train = X_train[atributos]\n","\n","      # Calculo da % para mostrar na tela\n","      percentagedisplay = round(100 - (len(list(X_train.columns))/tot2display)*100)\n","      # print(percentagedisplay,'% done...\\n')\n","      out.update(self.__progress(int(percentagedisplay), 100))\n","    #Remove possiveis nan\n","    resultTable.dropna(inplace=True)\n","    # print('100% done!\\n')\n","    out.update(self.__progress(100, 100))\n","    # Retorna a tabela com os resultados\n","    return resultTable\n","\n","  def __scoreAndSelection(self,resultTable,weightR2,weightProbF,weightBIC,weightRMSE):\n","    # Cria cópia da tabela original\n","    df_aux = resultTable.copy()\n","    df_aux.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    df_aux.dropna(inplace=True)\n","\n","    # Normaliza o R² ajustado\n","    df_aux['R² adj'] = MinMaxScaler().fit_transform(np.array(df_aux['R² adj']).reshape(-1,1))\n","\n","    # Aplica a transformação -log10(f) e então normaliza a estatistica f\n","    df_aux['F-statistic'] = [-math.log10(x) if x !=0 else sys.float_info.max for x in df_aux['F-statistic']]\n","    df_aux['F-statistic'] = MinMaxScaler().fit_transform(np.array(df_aux['F-statistic']).reshape(-1,1))\n","\n","    # Normaliza e então inverte a estatistica BIC (Quanto menor menor)\n","    df_aux['BIC'] = MinMaxScaler().fit_transform(np.array(df_aux['BIC']).reshape(-1,1))\n","    df_aux['BIC'] = [np.clip(1-x,0,1) for x in df_aux['BIC']]\n","\n","    # Normaliza 'MSE-CV' e inverte a estatistica MSE-cv\n","    df_aux['MSE-CV'] = MinMaxScaler().fit_transform(np.array(df_aux['MSE-CV']).reshape(-1,1))\n","    df_aux['MSE-CV'] = [np.clip(1-x,0,1) for x in df_aux['MSE-CV']]\n","\n","    # Cria coluna de Score\n","    df_aux['Score'] = [(r*weightR2)+(f*weightProbF)+(b*weightBIC)+(m*weightRMSE) for r,f,b,m in zip(df_aux['R² adj'],df_aux['F-statistic'],df_aux['BIC'],df_aux['MSE-CV'])]\n","\n","    # Encontra indice de maior Score\n","    indexSelected = list(df_aux.Score).index(max(list(df_aux.Score)))\n","\n","    # Seleciona atributos\n","    selected = df_aux.iloc[indexSelected].Atributos.split('@')\n","\n","    self.selected_taxa = selected # salva os atributos selecionados\n","\n","    retEstatisticas = list(resultTable.iloc[indexSelected][['R² adj','F-statistic','BIC','MSE-CV']])\n","\n","    self.results = {'R² adj':retEstatisticas[0],\n","                    'F-statistic':retEstatisticas[1],\n","                    'BIC':retEstatisticas[2],\n","                    'MSE-CV':retEstatisticas[3]} # Salva as estatisticas\n","\n","    retScore = df_aux.iloc[indexSelected].Score\n","\n","    self.score_best_model = retScore # Salva a pontuação do melhor modelo\n","\n","  def __write_results(self,path_out,name_append):\n","    # adiciona '/' caso n tneha\n","    if path_out != '':\n","      if path_out[-1]!= '/':\n","        path_out+='/'\n","    else:\n","      path_out = '/'.join(self.__path2MetaData.split('/')[:-1])+'/'\n","    # adiciona '_' caso n tenha\n","    if name_append != '':\n","      if name_append[0]!= '_':\n","        name_append = 'CODARFE_RESULTS_'+name_append\n","    else:\n","      name_append = 'CODARFE_RESULTS'\n","\n","    path2write = path_out +name_append+'.txt'\n","    print('Writing results at ',path2write)\n","\n","    with open(path2write,'w') as f:\n","      f.write('Results: \\n\\n')\n","      f.write('R² adj -> '+     str(self.results['R² adj'])+'\\n')\n","      f.write('F-statistic -> '+str(self.results['F-statistic'])+'\\n')\n","      f.write('BIC -> '+        str(self.results['BIC'])+'\\n')\n","      f.write('MSE-CV -> '+     str(self.results['MSE-CV'])+'\\n')\n","      f.write('Total of taxa selected -> '+str(len(self.selected_taxa))+'. This value corresponds to '+str((len(self.selected_taxa)/len(self.data.columns))*100)+'% of the total observed\\n\\n')\n","      f.write('Selected taxa: \\n\\n')\n","      f.write(','.join(self.selected_taxa))\n","\n","  def __DefineModel(self):\n","\n","      self.__model = RandomForestRegressor(n_estimators = 160, criterion = 'poisson',random_state=42)\n","      X = self.data[self.selected_taxa]\n","      X = self.__toCLR(X)\n","\n","      if np.std(self.target)/np.mean(self.target)>0.2:# Caso varie muitas vezes a média (ruido)\n","        targetLogTransformed = self.__calc_new_redimension(self.target) # Aplica transformação no alvo\n","        self.__model.fit(X,targetLogTransformed) # Treina com o alvo transformado\n","        self.__log_transform = True # Define flag de transformação\n","        #print('The target was log transformed!')\n","      else:\n","        self.__model.fit(X,self.target) # Treina um segundo modelo com o alvo como é\n","        self.__log_transform = False # Define flag de transformação\n","\n","  def __checkModelParams(self,\n","                         write_results,\n","                         path_out,\n","                         name_append,\n","                         rLowVar,\n","                         applyAbunRel,\n","                         percentage_cols_2_remove,\n","                         n_Kfold_CV,\n","                         weightR2,\n","                         weightProbF,\n","                         weightBIC,\n","                         weightRMSE,\n","                         n_max_iter_huber):\n","    if type(write_results) != bool:\n","      print(\"\\n\\nWARNING!\\n\\nwrite_results MUST be True or False\")\n","      return False\n","\n","    if type(rLowVar) != bool:\n","      print(\"\\n\\nWARNING!\\n\\nrLowVar MUST be True or False\")\n","      return False\n","\n","    if type(applyAbunRel) != bool:\n","      print(\"\\n\\nWARNING!\\n\\napplyAbunRel MUST be True or False\")\n","      return False\n","\n","    if type(path_out) != str:\n","      print(\"\\n\\nWARNING!\\n\\npath_out MUST be a String\")\n","      return False\n","\n","    elif write_results and path_out!='' and not os.path.exists(path_out):\n","      print('\\n\\nWARNING!\\n\\nThe path out does not exists!')\n","      return False\n","\n","    if type(name_append) != str:\n","      print(\"\\n\\nWARNING!\\n\\nname_append MUST be a String\")\n","      return False\n","\n","    if type(percentage_cols_2_remove) != int:\n","      print('\\n\\nWARNING!\\n\\npercentage_cols_2_remove MUST be a integer')\n","      return False\n","\n","    elif percentage_cols_2_remove < 1 or percentage_cols_2_remove > 99:\n","      print('\\n\\nWARNING!\\n\\npercentage_cols_2_remove MUST be between 1 and 99')\n","      return False\n","\n","    if type(n_Kfold_CV) != int:\n","      print('\\n\\nWARNING!\\n\\nn_Kfold_CV MUST be a integer')\n","      return False\n","\n","    elif n_Kfold_CV < 2 or n_Kfold_CV > 100:\n","      print('\\n\\nWARNING!\\n\\nn_Kfold_CV MUST be between 2 and 99')\n","      return False\n","\n","    if type(n_max_iter_huber) != int:\n","      print('\\n\\nWARNING!\\n\\nn_max_iter_huber MUST be a integer')\n","      return False\n","\n","    elif n_max_iter_huber < 2:\n","      print('\\n\\nWARNING!\\n\\nn_max_iter_huber MUST be greater than 2')\n","      return False\n","\n","    if type(weightR2) != float:\n","      print('\\n\\nWARNING!\\n\\nweightR2 MUST be a float')\n","      return False\n","\n","    elif weightR2 < 0:\n","      print('\\n\\nWARNING!\\n\\nweightR2 MUST be greater or equal to 0')\n","      return False\n","\n","    if type(weightProbF) != float:\n","      print('\\n\\nWARNING!\\n\\nweightProbF MUST be a float')\n","      return False\n","\n","    elif weightProbF < 0:\n","      print('\\n\\nWARNING!\\n\\nweightProbF MUST be greater or equal to 0')\n","      return False\n","\n","    if type(weightBIC) != float:\n","      print('\\n\\nWARNING!\\n\\nweightBIC MUST be a float')\n","      return False\n","\n","    elif weightBIC < 0:\n","      print('\\n\\nWARNING!\\n\\nweightBIC MUST be greater or equal to 0')\n","      return False\n","\n","    if type(weightRMSE) != float:\n","      print('\\n\\nWARNING!\\n\\nweightRMSE MUST be a float')\n","      return False\n","\n","    elif weightRMSE < 0:\n","      print('\\n\\nWARNING!\\n\\nweightRMSE MUST be greater or equal to 0')\n","      return False\n","\n","    return True\n","\n","  def CreateModel(self,\n","                  write_results: bool =True,\n","                  path_out: str ='',\n","                  name_append: str ='',\n","                  rLowVar: bool =True,\n","                  applyAbunRel: bool = True,\n","                  percentage_cols_2_remove: int =1,\n","                  n_Kfold_CV: int=10,\n","                  weightR2: int =1.0,\n","                  weightProbF: float=0.5,\n","                  weightBIC: float=1.0,\n","                  weightRMSE: float=1.5,\n","                  n_max_iter_huber: int=100):\n","\n","\n","    if type(self.data) == type(None):\n","      print('No data was provided!\\nPlease make sure to provide complete information or use the Load_Instance(<path_2_instance>) function to load an already created CODARFE model')\n","      return None\n","    print('\\n\\nChecking model parameters...')\n","    if not self.__checkModelParams(write_results,path_out,name_append,rLowVar,applyAbunRel,percentage_cols_2_remove,n_Kfold_CV,weightR2,weightProbF,weightBIC,weightRMSE,n_max_iter_huber):\n","      print('\\nPlease, Re-run this function with the correct parameters.')\n","      return None\n","    print('OK')\n","\n","    n_cols_2_remove = percentage_cols_2_remove/100\n","    self.__n_max_iter_huber = n_max_iter_huber # Define o numero de iterações utilziado pelo huber\n","\n","    if rLowVar:\n","      #Remove baixa variância\n","      self.__removeLowVar()\n","\n","    if applyAbunRel:\n","      #transforma em abundância relativa\n","      self.data = self.__toAbunRel(self.data)\n","\n","    method = HuberRegressor(epsilon = 2.0,alpha = 0.0003, max_iter = n_max_iter_huber)\n","\n","    # Remove iterativamente atributos enquanto cria vários modelos\n","    resultTable = self.__superRFE(method,n_cols_2_remove,n_Kfold_CV)\n","\n","    if len(resultTable)>0:\n","      # Calcula pontuação e seleciona o melhor modelo\n","      self.__scoreAndSelection(resultTable,weightR2,weightProbF,weightBIC,weightRMSE)\n","\n","      self.__DefineModel()\n","\n","      print('\\nModel created!\\n\\n')\n","      print('Results: \\n\\n')\n","      print('R² adj -> ',     self.results['R² adj'])\n","      print('F-statistic -> ',self.results['F-statistic'])\n","      print('BIC -> ',        self.results['BIC'])\n","      print('MSE-CV -> ',     self.results['MSE-CV'])\n","      print('Total of taxa selected -> ',len(self.selected_taxa),'. This value corresponds to ',(len(self.selected_taxa)/self.__totalPredictorsInDatabase)*100,'% of the total.\\n')\n","\n","      if write_results:\n","        self.__write_results(path_out,name_append)\n","\n","    else:\n","      print('The model was not able to generalize your Data.')\n","\n","  def __pairwise_correlation(self,A, B):\n","    am = A - np.mean(A, axis=0, keepdims=True)\n","    bm = B - np.mean(B, axis=0, keepdims=True)\n","    return am.T @ bm /  (np.sqrt(\n","        np.sum(am**2, axis=0,\n","               keepdims=True)).T * np.sqrt(\n","        np.sum(bm**2, axis=0, keepdims=True)))\n","\n","  def __CreateCorrelationImputer(self):\n","    threshold = 0.6 # Considered as strong correlation\n","    aux = self.__toCLR(self.data) # Remove composicionalidade usando CLR nos dados originais\n","\n","    for selected in self.selected_taxa: # Para cada taxa selecionada\n","      self.__correlation_list[selected] = [] # Cria instancia para esta taxa selecionada\n","      for taxa in aux.columns: # Verifica correlação com todas as outras\n","        if taxa != selected: # n comparar consigo mesmo\n","          corr = self.__pairwise_correlation(np.array(aux[selected]),np.array(aux[taxa]))# Calcula a correlação de forma rapida\n","          if corr >= threshold: # Somenta adiciona caso seja fortemente correlacionada\n","            self.__correlation_list[selected].append({'taxa':taxa,'corr':corr}) # Adiciona taxa correlacionada\n","      self.__correlation_list[selected].sort(reverse=True,key = lambda x: x['corr']) # Ordena pela correlação\n","\n","  def __Read_new_Data(self,path2Data):\n","    extension = path2Data.split('.')[-1]\n","    if extension == 'csv':\n","        data = pd.read_csv(path2Data,encoding='latin1')\n","        data.set_index(list(data.columns)[0],inplace=True)\n","    elif extension == 'tsv':\n","        data = pd.read_csv(path2Data,sep='\\t',encoding='latin1')\n","        data.set_index(list(data.columns)[0],inplace=True)\n","    elif extension == 'biom':\n","        table = load_table(path2Data)\n","        data = table.to_dataframe()\n","    elif extension == 'qza':\n","        output_directory =  '/'.join(path2Data.split('/')[:-1])+'/QZA_EXTRACT_CODARFE_TEMP/'\n","        # Openning the .qza file as an zip file\n","        with zipfile.ZipFile(path2Data, 'r') as zip_ref:\n","            # extracting all data to the output directory\n","            zip_ref.extractall(output_directory)\n","        # Getting the biom path file\n","        biompath = output_directory+os.listdir(output_directory)[0]+'/data/'\n","        biompath += [f for f in os.listdir(biompath) if f[-5:]=='.biom'][0]\n","        table = load_table(biompath)# Read the biom file\n","        data = table.to_dataframe() # Tranform it to a pandas dataframe\n","\n","        shutil.rmtree(output_directory) # remove the pathTree created\n","    return data\n","\n","  def Predict(self,\n","              path2newdata,\n","              applyAbunRel = True,\n","              writeResults = True,\n","              path_out = '',\n","              name_append = ''\n","              ):\n","    if self.__model == None:\n","      print('No model created, please run the function CreateModel first.')\n","      return None\n","\n","    if writeResults and path_out != '' and not os.path.exists(path_out):\n","      print('\\n\\nThe path out does not exists!\\nPlease try again with the correct path or let it blank to write in the same path as the metadata')\n","      return None\n","\n","    new = self.__Read_new_Data(path2newdata)\n","    newindex = new.index\n","\n","    if self.__correlation_list == {}:\n","      print('\\n\\nCreating correlation list for imputation method. It may take a few minutes depending on the size of the original dataset, but it will be create only once.\\n\\n')\n","      self.__CreateCorrelationImputer()\n","      print('Correlation list created!\\n\\n')\n","\n","    data2predict = pd.DataFrame() # Cria um dataframe para colocar apenas os dados selecionados\n","    totalNotFound = 0\n","    for selected in self.selected_taxa: # Para cada taxa selecionada\n","      if selected in new.columns: # Caso exista no novo conjunto\n","        data2predict[selected] = new[selected] # Adiciona o valor do novo conjunto no df de previsão\n","      else: # Senão\n","        found = False # Flag que indica se encontrou substitudo\n","        for correlated_2_selected in self.__correlation_list[selected]: # Para cada taxa correlacionada com a que não existe\n","          if correlated_2_selected['taxa'] in new.columns: # Caso encontre um substituto\n","            data2predict[selected] = new[correlated_2_selected['taxa']] # Coloca ele no lugar do que n existe\n","            found = True # Seta flag\n","            break\n","        if not found:\n","          data2predict[selected] = 0 # Caso não encontra retorna zero\n","          print('Warning! Taxa ',selected,' was not found and have no correlations! It may affect the model accuracy')\n","          totalNotFound+=1\n","\n","    if totalNotFound >= len(self.selected_taxa)*0.75:\n","      print('The new samples has less than 25% of selected taxa. The model will not be able to predict it.')\n","      return None,totalNotFound\n","\n","\n","    data2predict = data2predict.fillna(0)\n","\n","    if applyAbunRel:\n","      data2predict = self.__toAbunRel(data2predict) # Transforma em abundancia relativa\n","\n","    data2predict = self.__toCLR(data2predict) # Transforma para CLR\n","\n","    resp = self.__model.predict(data2predict)\n","\n","    if self.__log_transform: # Caso o modelo tenha sido treinado nos dados log transformados\n","      resp = self.__calc_inverse_redimension(resp)#,totalNotFound # Retorna os valores restaurados ao original\n","\n","    if writeResults:\n","\n","      if path_out != '':\n","        if path_out[-1]!= '/':\n","          path_out+='/Prediction'\n","      else:\n","        path_out = '/'.join(self.__path2MetaData.split('/')[:-1])+'/Prediction'\n","      if name_append != '':\n","        name_append = '_'+name_append\n","      filename = path_out+name_append+'.csv'\n","      pd.DataFrame(data = resp,columns = ['Prediction'],index=newindex).to_csv(filename)\n","\n","    return resp,totalNotFound\n","\n","  def Plot_Correlation(self,path_out='',name_append=''):\n","    if self.__model == None:\n","      print('No model created, please run the function CreateModel first.')\n","      return None\n","\n","    if path_out != '' and not os.path.exists(path_out):\n","      print('\\n\\nThe path out does not exists!\\nPlease try again with the correct path or let it blank to write in the same path as the metadata')\n","      return None\n","\n","    # Build a rectangle in axes coords\n","    left, width = .15, .75\n","    bottom, height = .15, .75\n","    right = left + width\n","    top = bottom + height\n","    y = self.target\n","    X = self.data[self.selected_taxa]\n","    X = self.__toCLR(X)\n","    pred = self.__model.predict(X)\n","\n","    if self.__log_transform: # Caso tenha aprendido com valores transformados\n","      pred = self.__calc_inverse_redimension(pred) # Destransforma-os\n","    plt.figure()\n","    plt.clf()\n","    ax = plt.gca()\n","\n","    corr, what = pearsonr(y, pred)\n","\n","    #Plota os pontos previsto por esperado\n","    plt.plot(pred, y, 'o')\n","\n","    # calcula o slop e intercept para uma regressão linear (plot da linha)\n","    m, b = np.polyfit(pred, y, 1)\n","\n","    #Adiciona a linha no plot\n","    plt.plot(pred, m*pred+b)\n","    shiftX = 0.2 * max(pred)\n","    shiftY = 0.1 * max(y)\n","\n","    ax.text(left, top, 'R = '+str(round(corr,2))+', p < '+str(round(what,2)),\n","          horizontalalignment='center',\n","          verticalalignment='center',\n","          transform=ax.transAxes)\n","\n","    if path_out != '':\n","      if path_out[-1]!= '/':\n","        path_out+='/'\n","    else:\n","      path_out = '/'.join(self.__path2MetaData.split('/')[:-1])+'/'\n","    # adiciona '_' caso n tenha\n","    if name_append != '':\n","      if name_append[0]!= '_':\n","        name_append = 'Correlation_'+name_append\n","    else:\n","      name_append = 'Correlation'\n","    filename = path_out+name_append+'.png'\n","\n","    print('\\nSaving the image in ',filename)\n","    plt.savefig(filename, dpi=600, bbox_inches='tight')\n","\n","  def __checkHoldOutParams(self,n_repetitions,test_size,path_out,name_append):\n","    if type(n_repetitions) != int:\n","      print('\\n\\nWARNING!\\n\\nn_repetitions MUST be a integer!')\n","      return False\n","    elif n_repetitions < 2:\n","      print('\\n\\nWARNING!\\n\\nn_repetitions MUST be at least 2!')\n","      return False\n","    if type(test_size) != int:\n","      print('\\n\\nWARNING!\\n\\ntest_size MUST be a integer!')\n","      return False\n","    elif test_size<=0:\n","      print('\\n\\nWARNING!\\n\\ntest_size MUST be greater than 0!')\n","      return False\n","    if path_out != '' and not os.path.exists(path_out):\n","      print('\\n\\nWARNING!\\n\\nThe path out does not exists!\\nPlease try again with the correct path or let it blank to write in the same path as the metadata')\n","      return False\n","\n","    return True\n","\n","  def Plot_HoldOut_Validation(self,\n","                              path_out='',\n","                              name_append='',\n","                              n_repetitions = 100,\n","                              test_size=20,\n","                              ):\n","    if self.__model == None:\n","      print('No model created, please run the function CreateModel first.')\n","      return None\n","    if not self.__checkHoldOutParams(n_repetitions,test_size,path_out,name_append):\n","      return None\n","    test_size = test_size/100\n","    method = RandomForestRegressor(n_estimators = 160, criterion = 'poisson',random_state=42)\n","    X = self.data[self.selected_taxa]\n","    X = self.__toCLR(X)\n","    y = self.target\n","    maes = []\n","    out = display(self.__progress(0, 100), display_id=True)\n","    for i in range(n_repetitions):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size) # divide em treino e teste\n","\n","      if self.__log_transform: # Caso tenha aprendido originalmente com valores transformados\n","        method.fit(X_train,self.__calc_new_redimension(y_train)) # Re-treina com os valores transformados\n","        y_pred = method.predict(X_test) # Realiza a predição\n","        y_pred = self.__calc_inverse_redimension(y_pred) # Destransforma-os\n","      else:\n","        method.fit(X_train,y_train)\n","        y_pred = method.predict(X_test)\n","\n","      # Calculo do MAE\n","      tt = 0\n","      for ii in range(len(y_pred)):\n","        tt+=abs(y_test.iloc[ii]-y_pred[ii])\n","      maes.append(tt/len(y_pred))\n","      out.update(self.__progress(int((i/n_repetitions)*100), 100))\n","\n","    sns.set_theme(style=\"ticks\")\n","    out.update(self.__progress(100, 100))\n","\n","    # Cria a figura zerada\n","    plt.figure()\n","    f, ax = plt.subplots(figsize=(7, 6))\n","\n","    # Plota o boxplot\n","    sns.boxplot(x=[1]*len(maes),\n","                y=maes,\n","                whis=[0, 100],\n","                width=.6,\n","                palette=\"vlag\")\n","\n","    # Adiciona os pontos sobre o boxplot\n","    sns.stripplot(x=[1]*len(maes),\n","                  y=maes,\n","                  size=4,\n","                  color=\".3\",\n","                  linewidth=0)\n","\n","    # Tweak the visual presentation\n","    ax.xaxis.grid(True)\n","    ax.set(ylabel=\"\")\n","    sns.despine(trim=True, left=True)\n","\n","    trainSize = int((1-test_size) *100)\n","    testSize = int(test_size*100)\n","    ax.set_title('Hold-out Validation ('+str(trainSize)+'-'+str(testSize)+') '+str(n_repetitions)+' repetitions',fontweight='bold')\n","    ax.set_ylabel('Mean Absolute Error')\n","    #ax.set_xlabel(target+' MAE')\n","\n","    if path_out != '':\n","      if path_out[-1]!= '/':\n","        path_out+='/'\n","    else:\n","      path_out = '/'.join(self.__path2MetaData.split('/')[:-1])+'/'\n","    # adiciona '_' caso n tenha\n","    if name_append != '':\n","      if name_append[0]!= '_':\n","        name_append = 'HoldOut_Validation_'+name_append\n","    else:\n","      name_append = 'HoldOut_Validation'\n","    filename = path_out+name_append+'.png'\n","\n","    print('\\nSaving the image in ',filename)\n","    plt.savefig(filename, dpi=600, bbox_inches='tight')\n","\n","\n","  def Plot_Relevant_Predictors(self,\n","                               path_out='',\n","                               name_append='',\n","                               n_max_features=100,\n","                               ):\n","\n","    if self.__model == None:\n","      print('No model created, please run the function CreateModel first.')\n","      return None\n","    if type(n_max_features) != int:\n","      print('\\n\\nWARNING!\\n\\nn_max_features MUST be a integer!')\n","      return None\n","    elif n_max_features<2:\n","      print('\\n\\nWARNING!\\n\\nn_max_features MUST be at least 2!')\n","      return None\n","    if path_out != '' and not os.path.exists(path_out):\n","      print('\\n\\nWARNING!\\n\\nThe path out does not exists!\\nPlease try again with the correct path or let it blank to write in the same path as the metadata')\n","      return None\n","    method = HuberRegressor(epsilon = 2.0,alpha = 0.0003, max_iter = self.__n_max_iter_huber)\n","    X = self.data[self.selected_taxa]\n","    X = self.__toCLR(X)\n","    y = self.target\n","    resp = method.fit(X,y)\n","\n","    dfaux = pd.DataFrame(data={'features':resp.feature_names_in_,'coefs':resp.coef_})\n","    dfaux.sort_values(by='coefs',ascending=False,inplace=True,ignore_index=True)\n","\n","    if len(dfaux) > n_max_features:\n","      half = int(n_max_features/2)\n","      totpos = len(dfaux.coefs[dfaux.coefs>0])\n","      totneg = len(dfaux.coefs[dfaux.coefs<0])\n","\n","      if totpos < half:\n","        totneg = half+half-totpos\n","      elif totneg < half:\n","        totpos = half+half-totneg\n","      else:\n","        totpos = half\n","        totneg = half\n","\n","      dfaux = dfaux[dfaux.index.isin([i for i in range(0,totpos)] + [i for i in range(len(dfaux)-totneg,len(dfaux))])]\n","    plt.figure()\n","    sns.set_theme(style=\"whitegrid\")\n","\n","    # Initialize the matplotlib figure\n","    f, ax = plt.subplots(figsize=(6, 15))#figsize=(6, 15)\n","\n","    colors = ['b' if x > 0 else 'r' for x in dfaux.coefs]\n","    # Plot the total crashes\n","    sns.set_color_codes(\"pastel\")\n","    sns.barplot(x=\"coefs\",\n","                y=\"features\",\n","                data=dfaux,\n","                palette=colors,\n","                )\n","\n","    ax.set_title('Strength of relevant predictors',fontweight='bold')\n","    ax.set(ylabel=\"Predictor name\",\n","          xlabel=\"Coefficient weight\")\n","    sns.despine(left=True, bottom=True)\n","\n","    if path_out != '':\n","      if path_out[-1]!= '/':\n","        path_out+='/'\n","    else:\n","      path_out = '/'.join(self.__path2MetaData.split('/')[:-1])+'/'\n","    # adiciona '_' caso n tenha\n","    if name_append != '':\n","      if name_append[0]!= '_':\n","        name_append = 'Relevant_Predictors_'+name_append\n","    else:\n","      name_append = 'Relevant_Predictors_'\n","    filename = path_out+name_append+'.png'\n","\n","\n","    print('\\nSaving the image in ',filename)\n","    plt.savefig(filename, dpi=600, bbox_inches='tight')\n","\n","\n","  # HEAT MAP (ps: eu n lembro de porra nenhuma de como eu criei isso... melhor n tentar otimizar nada)\n","  def __neatMapLinkage(self,selected_features):\n","    w = ca(selected_features)\n","    pc1 = w.features['CA1']\n","    pc2 = w.features['CA2']\n","\n","    xc = np.mean(pc1)\n","    yc = np.mean(pc2)\n","    theta = []\n","    for i in range(len(pc1)):\n","      theta.append(math.atan2(pc2[i] - yc, pc1[i] - xc ))\n","    order = [index for index, element in sorted(enumerate(theta), key=operator.itemgetter(1))]\n","    names = [selected_features.columns[i] for i in order]\n","    return names\n","\n","  def __heatmap(self,data, row_labels, col_labels, ax=None,cbar_kw={}, cbarlabel=\"\", **kwargs):\n","\n","    if not ax:\n","        ax = plt.gca()\n","\n","    # Plot the heatmap\n","    im = ax.imshow(data, **kwargs)\n","\n","    # Create colorbar\n","    divider = make_axes_locatable(ax)\n","    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n","\n","    cbar = ax.figure.colorbar(im, ax=ax,cax=cax, **cbar_kw)#im\n","    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n","\n","    # Show all ticks and label them with the respective list entries.\n","    ax.set_xticklabels(labels=col_labels)\n","    ax.set_yticklabels(labels=row_labels)\n","    ax.set_xticks(np.arange(len(col_labels)))\n","    ax.set_yticks(np.arange(len(row_labels)))\n","    # Let the horizontal axes labeling appear on top.\n","    ax.tick_params(top=True, bottom=False,\n","                    labeltop=True, labelbottom=False)\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=-90, ha=\"right\",\n","              rotation_mode=\"anchor\")\n","\n","    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n","    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n","    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n","    ax.tick_params(which=\"minor\", bottom=False, left=False)\n","\n","    return im, cbar\n","\n","  def Plot_Heatmap(self,\n","                   path_out='',\n","                   name_append=''):\n","\n","    if self.__model == None:\n","      print('No model created, please run the function CreateModel first.')\n","      return None\n","    if path_out != '' and not os.path.exists(path_out):\n","      print('\\n\\nWARNING!\\n\\nThe path out does not exists!\\nPlease try again with the correct path or let it blank to write in the same path as the metadata')\n","      return None\n","    # Pega o dataframe original porem apenas o que foi selecioando\n","    selected_features = self.data[self.selected_taxa]\n","\n","    # Clusterizando bacterias\n","    y = self.target\n","\n","    ###### Aqui clusteriza por CA ############\n","    leaf_names = self.__neatMapLinkage(selected_features)\n","    ##########################################\n","    clustered_df = pd.DataFrame()\n","\n","    for name in leaf_names:\n","      clustered_df[name] = selected_features[name]\n","    clustered_df['Target'] = y\n","    selected_features = clustered_df\n","\n","    # Ordenando bacterias por variável alvo\n","    selected_features_t = selected_features.T\n","    sorted_t = selected_features_t.sort_values(by='Target',axis=1,ascending=False)\n","    y = list(sorted_t.iloc[-1])\n","\n","    # Separando os dados para o plot\n","    bac_counts = sorted_t.drop('Target',axis=0).replace(0,0.5).values\n","\n","    bacs = list(sorted_t.drop('Target',axis=0).index[:])\n","\n","    # Aplica o CLR\n","    bac_clr = clr(bac_counts+0.001)\n","    vmin = min(bac_clr.flatten())\n","    vmax = max(bac_clr.flatten())\n","    norm = colors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n","\n","    Largura = int(len(y)*0.2)\n","    Altura  = int(len(bac_counts)*0.2)\n","    if Largura < 15:\n","      Largura = 15\n","    if Altura < 20:\n","      Altura = 20\n","    if Largura > 150:\n","      Largura = 150\n","    if Altura > 200:\n","      Altura = 200\n","    plt.figure()\n","    fig, ax = plt.subplots(figsize=(Largura,Altura))\n","\n","    im, cbar = self.__heatmap(bac_clr, bacs, y, ax=ax, cmap=\"RdYlBu\",norm = norm, cbarlabel=\"Center-Log-Ratio\")\n","\n","    fig.tight_layout()\n","\n","\n","    if path_out != '':\n","      if path_out[-1]!= '/':\n","        path_out+='/'\n","    else:\n","      path_out = '/'.join(self.__path2MetaData.split('/')[:-1])+'/'\n","    # adiciona '_' caso n tenha\n","    if name_append != '':\n","      if name_append[0]!= '_':\n","        name_append = 'HeatMap_'+name_append\n","    else:\n","      name_append = 'HeatMap'\n","    filename = path_out+name_append+'.png'\n","\n","\n","    print('\\nSaving the image in ',filename)\n","    plt.savefig(filename, dpi=600, bbox_inches='tight')\n"]},{"cell_type":"markdown","metadata":{"id":"_qiFMUzFTPUN"},"source":["# Example"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oys1XFOoj3FZ"},"outputs":[],"source":["# What I do recoment if you are working with google colab is to create a variable (such as 'basepath') to hold the base path for your data\n","# and use it + the name you want to save a file.\n","\n","basepath = 'base/path/out'\n","coda = CODARFE(path2Data = 'path/2/your/data',\n","               path2MetaData = 'path/2/your/meta/data',\n","               metaData_Target = 'target')\n","#or\n","#coda.Load_Instance() # MUST provide path to '*.foda' file\n","coda.CreateModel(write_results=True,\n","                  path_out = basepath,\n","                  name_append ='',\n","                  rLowVar=True,\n","                  applyAbunRel= True,\n","                  percentage_cols_2_remove =1,\n","                  n_Kfold_CV=10,\n","                  weightR2 =1.0,\n","                  weightProbF=0.5,\n","                  weightBIC=1.0,\n","                  weightRMSE=1.5,\n","                  n_max_iter_huber=100)\n","\n","coda.Save_Instance(basepath) # default name is CODARFE_RESULTS\n","coda.Predict('path/2/your/new/samples') # MUST provide the path to the new samples\n","\n","coda.Plot_Correlation(basepath) # Displays and saves the predicted x expected correction plot of the triene\n","coda.Plot_HoldOut_Validation(basepath) # Performs hold out validation 25 times and displays the MAE boxplot\n","coda.Plot_Relevant_Predictors(basepath) # Display and save selected predictors with their correlation strength\n","coda.Plot_Heatmap(basepath) # Display and save the heatmap\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}